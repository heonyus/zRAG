# mz-RAG: LLM-as-Memory

> **Replacing Vector DB with Internal LLM Memory**

This project replaces the traditional "Retriever + Vector DB" pipeline in RAG systems with **learnable memory vectors** inside a Local LLM to generate evidence directly.

## Core Idea

```
Traditional RAG:
Query â†’ [Retriever] â†’ Vector DB â†’ Retrieved Text â†’ LLM â†’ Answer
              â†‘
        External Module

LLM-as-Memory (Ours):
Query â†’ [Local LLM + Internal Memory Z] â†’ Evidence â†’ ChatGPT â†’ Answer
                    â†‘
              No External Module
              Internal routing selects z
```

### Key Differences

| Traditional RAG | LLM-as-Memory |
|-----------------|---------------|
| Requires external retriever | **No retriever** |
| Stores text in Vector DB | **Compressed into z vectors** |
| Embedding similarity search | **LLM internal attention routing** |
| Retrieved text â†’ LLM â†’ Answer | **Evidence generation â†’ ChatGPT â†’ Answer** |

## Phased Approach

| Phase | Name | Goal | Status |
|-------|------|------|--------|
| **1** | Write (Token Learning) | z_i â†’ D_i generation | âœ… Completed |
| **1.5** | Evidence Generation | z_i â†’ Evidence (answer-containing) | âœ… Completed |
| **2** | Read - Option A | [Z_all] + query â†’ evidence | ðŸ”„ In Progress |
| **3** | Read - Option B-1 | KV Injection (scale to N=2000+) | ðŸ“‹ Planned |
| **4** | Read - Option B-2 | Resampler (scale to N=10000+) | ðŸ“‹ Planned |

---

## Project Structure (Full)

```
zRAG/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ pyproject.toml            # Project dependencies (uv/pip)
â”œâ”€â”€ uv.lock                   # Locked dependencies
â”‚
â”œâ”€â”€ models/                   # Model architectures
â”‚   â”œâ”€â”€ write_phase_model.py      # Phase 1: z_i â†’ D_i
â”‚   â”œâ”€â”€ parametric_memory_llm.py  # Phase 2: Z_all concat â†’ evidence
â”‚   â””â”€â”€ evidence_trainer.py       # Trainer for evidence generation
â”‚
â”œâ”€â”€ training/                 # Training scripts
â”‚   â”œâ”€â”€ train_write_phase.py      # Phase 1 training
â”‚   â””â”€â”€ train_evidence.py         # Phase 2 training
â”‚
â”œâ”€â”€ data/                     # Data processing
â”‚   â”œâ”€â”€ dataloader.py             # WritePhaseDataset, ReadPhaseDataset
â”‚   â”œâ”€â”€ download.py               # Dataset download (NQ, HotpotQA)
â”‚   â”œâ”€â”€ corpus_builder.py         # Build corpus from HotpotQA
â”‚   â”œâ”€â”€ build_phase1_5_evidence_dataset.py  # Phase 1.5 dataset builder
â”‚   â””â”€â”€ raw/                      # [GITIGNORED] Raw downloaded data
â”‚
â”œâ”€â”€ configs/                  # Configuration files
â”‚   â”œâ”€â”€ phase1_write.yaml         # Phase 1 config
â”‚   â”œâ”€â”€ phase1_5.yaml             # Phase 1.5 config
â”‚   â””â”€â”€ evidence_poc.yaml         # Phase 2 config
â”‚
â”œâ”€â”€ evaluation/               # Evaluation scripts
â”‚   â””â”€â”€ evidence_metrics.py       # ROUGE-L, Answer Coverage
â”‚
â”œâ”€â”€ experiments/              # Experiment runners
â”‚   â”œâ”€â”€ phase1_analysis/          # Phase 1 ablation study
â”‚   â””â”€â”€ phase1_5/                 # Phase 1.5 experiment runner
â”‚
â”œâ”€â”€ baselines/                # Baseline comparisons
â”‚   â””â”€â”€ standard_rag.py           # BM25/Dense RAG baseline
â”‚
â”œâ”€â”€ docs/                     # Documentation
â”‚   â””â”€â”€ phases/                   # Detailed phase documentation
â”‚
â”œâ”€â”€ checkpoints/              # [GITIGNORED] Model checkpoints
â”œâ”€â”€ results/                  # [GITIGNORED] Experiment results
â””â”€â”€ logs/                     # [GITIGNORED] Training logs
```

---

## Gitignored Data Structure (Backup Required)

These folders are NOT in GitHub and must be downloaded separately.

### Download Link
**Backup file**: `zRAG_backup.tar.gz` (1.7GB)

### Extraction
```bash
# Extract to project root
cd /path/to/zRAG
tar -xzvf zRAG_backup.tar.gz
```

### Folder Structure After Extraction

```
zRAG/
â”œâ”€â”€ checkpoints/                          # 401MB - Model weights
â”‚   â”‚
â”‚   â”œâ”€â”€ phase1_final/                     # Phase 1 initial experiment
â”‚   â”‚   â”œâ”€â”€ z_pool.pt                     # Final z vectors (50 docs Ã— 4 tokens Ã— 3584 dim)
â”‚   â”‚   â”œâ”€â”€ z_pool_epoch{10,20,30,40,50}.pt  # Checkpoints per epoch
â”‚   â”‚   â”œâ”€â”€ projection.pt                 # Projection layer weights
â”‚   â”‚   â”œâ”€â”€ results.pt                    # Training results
â”‚   â”‚   â”œâ”€â”€ corpus_manifest.json          # Corpus metadata
â”‚   â”‚   â”œâ”€â”€ config.yaml                   # Training config
â”‚   â”‚   â””â”€â”€ logs/                         # Training logs
â”‚   â”‚
â”‚   â”œâ”€â”€ phase1_v2/                        # Phase 1 v2 (main experiment)
â”‚   â”‚   â”œâ”€â”€ z_pool.pt                     # Final z vectors
â”‚   â”‚   â”œâ”€â”€ z_pool_epoch{10,20,30,40,50}.pt
â”‚   â”‚   â”œâ”€â”€ projection.pt
â”‚   â”‚   â”œâ”€â”€ results.pt
â”‚   â”‚   â”œâ”€â”€ corpus_manifest.json
â”‚   â”‚   â””â”€â”€ logs/
â”‚   â”‚
â”‚   â”œâ”€â”€ phase2_corpus/                    # Corpus for Phase 2
â”‚   â”‚   â”œâ”€â”€ corpus.json                   # 200 documents
â”‚   â”‚   â”œâ”€â”€ qa_pairs.json                 # All QA pairs
â”‚   â”‚   â”œâ”€â”€ qa_train.json                 # Training split
â”‚   â”‚   â”œâ”€â”€ qa_val.json                   # Validation split
â”‚   â”‚   â””â”€â”€ stats.json                    # Corpus statistics
â”‚   â”‚
â”‚   â”œâ”€â”€ phase2_read/                      # Phase 2 Read experiment
â”‚   â”‚   â”œâ”€â”€ best.pt                       # Best checkpoint
â”‚   â”‚   â”œâ”€â”€ best.pt_lora/                 # LoRA adapter
â”‚   â”‚   â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â”‚   â”‚   â””â”€â”€ adapter_config.json
â”‚   â”‚   â”œâ”€â”€ final.pt
â”‚   â”‚   â”œâ”€â”€ final.pt_lora/
â”‚   â”‚   â”œâ”€â”€ results.pt
â”‚   â”‚   â””â”€â”€ samples.json
â”‚   â”‚
â”‚   â””â”€â”€ phase2_cache/                     # Baseline comparison cache
â”‚       â”œâ”€â”€ answer_bm25.jsonl
â”‚       â”œâ”€â”€ answer_contriever.jsonl
â”‚       â”œâ”€â”€ answer_dense_e5.jsonl
â”‚       â”œâ”€â”€ answer_zRAG.jsonl
â”‚       â”œâ”€â”€ evidence_*.jsonl
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ results/                              # 1.4GB - Experiment results
â”‚   â”‚
â”‚   â”œâ”€â”€ phase1_analysis/                  # Phase 1 ablation study
â”‚   â”‚   â””â”€â”€ 20260128_123048/
â”‚   â”‚       â”œâ”€â”€ 00_meta/                  # Run metadata
â”‚   â”‚       â”‚   â”œâ”€â”€ effective_config.json
â”‚   â”‚       â”‚   â””â”€â”€ run_manifest.json
â”‚   â”‚       â”œâ”€â”€ 00_logs/                  # Logs
â”‚   â”‚       â”‚   â”œâ”€â”€ run.log
â”‚   â”‚       â”‚   â””â”€â”€ debug.log
â”‚   â”‚       â”œâ”€â”€ 01_verification/          # A1, A3 tests
â”‚   â”‚       â”‚   â”œâ”€â”€ A1_confusion/         # Confusion matrix analysis
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ confusion_metrics.json
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ nll_matrix.npy
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ artifacts/        # Visualizations
â”‚   â”‚       â”‚   â””â”€â”€ A3_zshuffle/          # Z shuffle test
â”‚   â”‚       â”‚       â”œâ”€â”€ z_shuffle_metrics.json
â”‚   â”‚       â”‚       â””â”€â”€ artifacts/
â”‚   â”‚       â”œâ”€â”€ 02_ablations/             # Ablation studies
â”‚   â”‚       â”‚   â””â”€â”€ B2_projection/
â”‚   â”‚       â”‚       â”œâ”€â”€ proj_normal/
â”‚   â”‚       â”‚       â”œâ”€â”€ proj_frozen_ckpt/
â”‚   â”‚       â”‚       â””â”€â”€ proj_random_frozen/
â”‚   â”‚       â””â”€â”€ 03_summary/               # Summary dashboard
â”‚   â”‚           â”œâ”€â”€ dashboard.md
â”‚   â”‚           â””â”€â”€ ablation_summary.json
â”‚   â”‚
â”‚   â””â”€â”€ phase1_5/                         # Phase 1.5 experiments
â”‚       â””â”€â”€ 20260128_184412/              # Latest run (MAIN RESULT)
â”‚           â”œâ”€â”€ 00_meta/
â”‚           â”‚   â”œâ”€â”€ effective_config.json # Experiment config
â”‚           â”‚   â””â”€â”€ run_manifest.json
â”‚           â”œâ”€â”€ 00_logs/
â”‚           â”‚   â”œâ”€â”€ run.log
â”‚           â”‚   â””â”€â”€ debug.log
â”‚           â”œâ”€â”€ 01_data/
â”‚           â”‚   â”œâ”€â”€ dataset.jsonl         # Training data
â”‚           â”‚   â”œâ”€â”€ dataset_manifest.json # Data statistics
â”‚           â”‚   â””â”€â”€ samples_preview.md    # Sample preview
â”‚           â”œâ”€â”€ 02_train/
â”‚           â”‚   â”œâ”€â”€ train_summary.json    # Training summary
â”‚           â”‚   â”œâ”€â”€ train_metrics.jsonl   # Per-step metrics
â”‚           â”‚   â”œâ”€â”€ frozen_params_verification.json
â”‚           â”‚   â”œâ”€â”€ checkpoints/
â”‚           â”‚   â”‚   â”œâ”€â”€ best.pt_lora/     # Best LoRA checkpoint
â”‚           â”‚   â”‚   â””â”€â”€ last.pt_lora/     # Final LoRA checkpoint
â”‚           â”‚   â””â”€â”€ artifacts/
â”‚           â”‚       â”œâ”€â”€ loss_curve.png
â”‚           â”‚       â””â”€â”€ loss_curve.pdf
â”‚           â”œâ”€â”€ 03_eval/
â”‚           â”‚   â”œâ”€â”€ evidence_metrics.json # Main evaluation metrics
â”‚           â”‚   â”œâ”€â”€ evidence_eval_table.csv
â”‚           â”‚   â”œâ”€â”€ samples/
â”‚           â”‚   â”‚   â”œâ”€â”€ eyeball_20_best.md    # Best samples
â”‚           â”‚   â”‚   â”œâ”€â”€ eyeball_20_worst.md   # Worst samples
â”‚           â”‚   â”‚   â””â”€â”€ failure_cases.md      # Failures (coverage=0)
â”‚           â”‚   â””â”€â”€ artifacts/
â”‚           â”œâ”€â”€ 04_regression_phase1/
â”‚           â”‚   â”œâ”€â”€ baseline_metrics.json     # Phase 1 baseline
â”‚           â”‚   â”œâ”€â”€ post_phase15_metrics.json # After Phase 1.5
â”‚           â”‚   â”œâ”€â”€ delta.json                # Regression test result
â”‚           â”‚   â””â”€â”€ artifacts/
â”‚           â””â”€â”€ 05_cache/
â”‚               â””â”€â”€ eval_results.jsonl
â”‚
â”œâ”€â”€ logs/                                 # 32KB - Additional logs
â”‚   â””â”€â”€ phase2_read/
â”‚       â””â”€â”€ train_read_v1.log
â”‚
â””â”€â”€ data/
    â””â”€â”€ raw/                              # Raw HotpotQA data
        â””â”€â”€ hotpot_dev_distractor_v1.json
```

---

## Key Results (Phase 1.5 - 2026-01-28)

### Training Configuration
| Parameter | Value |
|-----------|-------|
| Base Model | Qwen3-8B (frozen) |
| Fine-tuning | LoRA (r=32, alpha=64) |
| Training Samples | 58 |
| Epochs | 10 |
| Final Loss | 0.348 |

### Evaluation Metrics
| Metric | Value | Description |
|--------|-------|-------------|
| **Answer Coverage** | 81.0% | Generated evidence contains gold answer |
| **Source Overlap** | 43.2% | Overlap with original document |
| **ROUGE-L** | 38.3% | Text similarity |
| **4-gram Overlap** | 19.6% | Exact phrase match |

### Regression Test (Phase 1 Retrieval)
| Test | Result | Threshold |
|------|--------|-----------|
| A1 (Top-1 Drop) | 0.0% | < 2% |
| A3 (Shuffle Delta) | 0.98 | > 0.5 |
| **Overall** | âœ… PASS | - |

### Known Issues
- 11 failure cases (Answer Coverage = 0)
- Hallucination: Model generates factually incorrect details
- Paraphrasing: Low source overlap indicates heavy paraphrasing

---

## Quick Start

### Installation

```bash
# Using uv (recommended)
curl -LsSf https://astral.sh/uv/install.sh | sh
cd zRAG
uv sync

# Activate virtual environment
source .venv/bin/activate  # Linux/macOS
.\.venv\Scripts\Activate.ps1  # Windows PowerShell
```

### Download Checkpoints

```bash
# Download and extract backup
# (Get zRAG_backup.tar.gz from shared storage)
tar -xzvf zRAG_backup.tar.gz
```

### Run Phase 1 Training

```bash
python training/train_write_phase.py --config configs/phase1_write.yaml
```

### Run Phase 1.5 Experiment

```bash
python experiments/phase1_5/run_phase1_5.py --config configs/phase1_5.yaml
```

---

## Hardware Requirements

| Phase | GPU Memory | Notes |
|-------|------------|-------|
| Phase 1 | ~8GB | Single doc at a time |
| Phase 1.5 | ~16GB | LoRA fine-tuning |
| Phase 2 (N=200) | ~12GB | Z prefix = 800 tokens |
| Phase 2 (N=2000) | ~24GB | Z prefix = 8000 tokens |

**Recommended**: NVIDIA L4 24GB or A100 40GB

---

## References

- Soft Prompt: Prompt Tuning, P-tuning v2, Prefix Tuning
- Generative Retrieval: DSI, DSI++
- Document Compression: Gist Tokens, ICAE, xRAG
- Parametric RAG: DyPRAG

## License

MIT
