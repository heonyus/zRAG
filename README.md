# zRAG: LLM-as-Memory

> **Replacing Vector DB with Internal LLM Memory**

This project replaces the traditional "Retriever + Vector DB" pipeline in RAG systems with **learnable memory vectors** inside a Local LLM to generate evidence directly.

## Core Idea

```
Traditional RAG:
Query â†’ [Retriever] â†’ Vector DB â†’ Retrieved Text â†’ LLM â†’ Answer
              â†‘
        External Module

LLM-as-Memory (Ours):
Query â†’ [Local LLM + Internal Memory Z] â†’ Evidence â†’ ChatGPT â†’ Answer
                    â†‘
              No External Module
              Internal routing selects z
```

### Key Differences

| Traditional RAG | LLM-as-Memory |
|-----------------|---------------|
| Requires external retriever | **No retriever** |
| Stores text in Vector DB | **Compressed into z vectors** |
| Embedding similarity search | **LLM internal attention routing** |
| Retrieved text â†’ LLM â†’ Answer | **Evidence generation â†’ ChatGPT â†’ Answer** |

## Phased Approach

| Phase | Name | Goal | Status |
|-------|------|------|--------|
| **1** | Write (Token Learning) | z_i â†’ D_i generation | âœ… Implemented |
| **2** | Read - Option A | [Z_all] + query â†’ evidence | ğŸ”„ Ready |
| **3** | Read - Option B-1 | KV Injection (scale to N=2000+) | ğŸ“‹ Planned |
| **4** | Read - Option B-2 | Resampler (scale to N=10000+) | ğŸ“‹ Planned |

## Architecture

### Phase 1: Write (Token-as-Document)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Local LLM (Qwen3-8B) - FROZEN            â”‚
â”‚                                                             â”‚
â”‚   z_i (learnable tokens)                                    â”‚
â”‚        â”‚                                                    â”‚
â”‚        â–¼                                                    â”‚
â”‚   [Projection] â†’ LLM â†’ Document D_i                         â”‚
â”‚                                                             â”‚
â”‚   Loss: -log P(D_i | z_i)                                   â”‚
â”‚   Learn: z_i only (LLM frozen)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 3: Read (Z_all Concat)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Local LLM (Qwen3-8B)                     â”‚
â”‚                                                             â”‚
â”‚   Memory Pool: Z = {zâ‚, zâ‚‚, ..., zâ‚™}                       â”‚
â”‚   (N docs Ã— 4 tokens Ã— 256 dim = learnable vectors)        â”‚
â”‚                                                             â”‚
â”‚   [Z_all prefix] + Query â†’ LLM â†’ Evidence                   â”‚
â”‚                                                             â”‚
â”‚   Internal attention routes to relevant z                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
              Query + Evidence â†’ ChatGPT â†’ Answer
```

## Quick Start

### Installation

```bash
# Using uv (recommended)
curl -LsSf https://astral.sh/uv/install.sh | sh
cd zrag
uv sync

# Or using pip
pip install -e .
```

### Phase 1: Write Phase Training

```bash
# Full training (200 docs, 100 epochs per doc)
python training/train_write_phase.py --config configs/phase1_write.yaml

# Quick test (10 docs, 20 epochs per doc)
python training/train_write_phase.py --config configs/phase1_write.yaml --test
```

### Phase 3: Read Phase Training

```bash
# After Phase 1 completes, load z_pool and train evidence generation
python training/train_evidence.py --config configs/evidence_poc.yaml
```

## Project Structure

```
zrag/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ write_phase_model.py     # Phase 1: z_i â†’ D_i (NEW)
â”‚   â”œâ”€â”€ parametric_memory_llm.py # Phase 3: Z_all concat â†’ evidence
â”‚   â””â”€â”€ evidence_trainer.py      # Trainer for evidence generation
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_write_phase.py     # Phase 1 training (NEW)
â”‚   â””â”€â”€ train_evidence.py        # Phase 3 training
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataloader.py            # WritePhaseDataset, ReadPhaseDataset
â”‚   â””â”€â”€ download.py              # Dataset download (NQ, HotpotQA)
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ phase1_write.yaml        # Phase 1 config (NEW)
â”‚   â””â”€â”€ evidence_poc.yaml        # Phase 3 config
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ evidence_metrics.py      # ROUGE-L, Answer Coverage
â”œâ”€â”€ baselines/
â”‚   â””â”€â”€ standard_rag.py          # BM25/Dense RAG baseline
â””â”€â”€ docs/
    â””â”€â”€ phases/                  # Detailed phase documentation
```

## Configuration

### Phase 1 (Write)

| Parameter | Value | Description |
|-----------|-------|-------------|
| `num_docs` | 200 | Documents to learn |
| `m_tokens` | 4 | Memory tokens per document |
| `z_dim` | 256 | Memory vector dimension |
| `epochs_per_doc` | 100 | Training epochs per document |
| `LLM` | Qwen3-8B | Frozen (no LoRA) |

### Phase 3 (Read)

| Parameter | Value | Description |
|-----------|-------|-------------|
| `num_docs` | 200 | Load from Phase 1 |
| `m_tokens` | 4 | Memory tokens per document |
| `z_dim` | 256 | Memory vector dimension |
| `LLM` | Qwen3-8B | QLoRA fine-tuning |

## Hardware Requirements

| Phase | GPU Memory | Notes |
|-------|------------|-------|
| Phase 1 | ~8GB | Single doc at a time |
| Phase 3 (N=200) | ~12GB | Z prefix = 800 tokens |
| Phase 3 (N=500) | ~16GB | Z prefix = 2000 tokens |
| Phase 3 (N=2000) | ~24GB | Z prefix = 8000 tokens |

**Recommended**: NVIDIA L4 24GB (GCP g2-standard-4)

## Phase 1 â†’ Phase 3 Workflow

```python
# Phase 1: Train z_i for each document
# Output: checkpoints/phase1_write/z_pool.pt

# Phase 3: Load trained z_pool
from models import ParametricMemoryLLM

model = ParametricMemoryLLM(num_docs=200, m_tokens=4, z_dim=256, ...)
model.load_from_phase1(
    z_pool_path="checkpoints/phase1_write/z_pool.pt",
    projection_path="checkpoints/phase1_write/projection.pt"
)

# Continue training for evidence generation
```

## References

- Soft Prompt: Prompt Tuning, P-tuning v2, Prefix Tuning
- Generative Retrieval: DSI, DSI++
- Document Compression: Gist Tokens, ICAE, xRAG
- Parametric RAG: DyPRAG

## License

MIT
