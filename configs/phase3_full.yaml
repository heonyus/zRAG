# Phase 3: Full Evaluation
# 목표: 7개 Baseline과 공정 비교

model:
  llm_name: "Qwen/Qwen3-8B"
  quantization: "4bit"
  lora:
    r: 64
    alpha: 128
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
    dropout: 0.05

parametric_qa:
  num_docs: 50000
  z_dim: 256  # Phase 2에서 결정
  m_tokens: 8  # Phase 2에서 결정
  selection_method: "learned_router"  # Phase 2에서 결정

data:
  dataset: "natural_questions"
  corpus_size: 50000
  train_samples: null  # full train set
  eval_samples: null  # full test set (3610)
  max_doc_length: 512
  max_query_length: 128
  retrieval_count: 5  # FlashRAG standard

write_phase:
  epochs: 50
  lr: 1e-3
  llm_frozen: true
  batch_size: 8

read_phase:
  epochs: 3
  lr_llm: 2e-5
  lr_z: 1e-3
  lr_proj: 1e-4
  alpha_retrieval: 0.1
  batch_size: 2
  gradient_accumulation: 8
  warmup_ratio: 0.1
  top_k: 5

baselines:
  - name: "no_retrieval"
    type: "no_retrieval"
  - name: "bm25"
    type: "bm25"
    retriever: "bm25"
  - name: "standard_rag"
    type: "standard_rag"
    retriever: "e5-base-v2"
  - name: "self_rag"
    type: "flashrag"
    method: "self_rag"
  - name: "ircot"
    type: "flashrag"
    method: "ircot"
  - name: "adaptive_rag"
    type: "flashrag"
    method: "adaptive_rag"
  - name: "corag"
    type: "corag"
    checkpoint: "corag/corag-llama3.1-8b"

evaluation:
  metrics: ["em", "f1", "recall_at_k", "mrr", "latency", "memory", "storage"]
  ragas_metrics: ["faithfulness", "answer_relevancy", "context_precision", "context_recall"]
  seeds: [42, 123, 456]
  report_mean_std: true

hardware:
  device: "cuda"
  fp16: true
  gradient_checkpointing: true

logging:
  project: "parametric-qa"
  run_name: "phase3-full"
  use_wandb: true
  log_dir: "./logs/phase3"
  save_dir: "./checkpoints/phase3"
