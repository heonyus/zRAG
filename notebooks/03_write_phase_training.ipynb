{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Write Phase Training\n",
    "\n",
    "Stage 1: Document → z_i 학습 (Document Reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\".\").resolve().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "import yaml\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config 로드\n",
    "config_path = PROJECT_ROOT / \"configs\" / \"phase1_poc.yaml\"\n",
    "with open(config_path) as f:\n",
    "    config = OmegaConf.create(yaml.safe_load(f))\n",
    "\n",
    "# Write phase config\n",
    "write_config = config.write_phase\n",
    "print(\"Write Phase Configuration:\")\n",
    "print(OmegaConf.to_yaml(write_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "BATCH_SIZE = write_config.batch_size\n",
    "LEARNING_RATE = write_config.lr_z\n",
    "NUM_EPOCHS = write_config.epochs\n",
    "MAX_DOC_LENGTH = 512\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate (z): {LEARNING_RATE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataloader import WritePhaseDataset\n",
    "import json\n",
    "\n",
    "# 샘플 corpus (실제로는 전처리된 데이터 로드)\n",
    "sample_corpus = {\n",
    "    0: \"France is a country in Western Europe. Paris is the capital and largest city.\",\n",
    "    1: \"Romeo and Juliet is a tragedy written by William Shakespeare.\",\n",
    "    2: \"The Great Wall of China is a series of fortifications made of various materials.\",\n",
    "    3: \"Albert Einstein developed the theory of relativity, one of the two pillars of modern physics.\",\n",
    "    4: \"The Amazon rainforest produces about 20% of the world's oxygen.\",\n",
    "}\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model.llm_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Dataset\n",
    "write_dataset = WritePhaseDataset(sample_corpus, tokenizer, max_length=MAX_DOC_LENGTH)\n",
    "write_loader = DataLoader(write_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Corpus size: {len(sample_corpus)}\")\n",
    "print(f\"Dataset size: {len(write_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.parametric_qa import ParametricQA\n",
    "\n",
    "# 디바이스 설정\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델 초기화 (메모리가 충분할 때만)\n",
    "# model = ParametricQA(\n",
    "#     llm_name=config.model.llm_name,\n",
    "#     num_docs=len(sample_corpus),\n",
    "#     z_dim=config.parametric_qa.z_dim,\n",
    "#     m_tokens=config.parametric_qa.m_tokens,\n",
    "#     selection_method=config.parametric_qa.selection_method,\n",
    "#     lora_r=config.model.lora.r,\n",
    "#     lora_alpha=config.model.lora.alpha,\n",
    "#     use_4bit=config.model.use_4bit,\n",
    "# )\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write Phase Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WritePhaseTrainer:\n",
    "    \"\"\"\n",
    "    Write Phase Training\n",
    "    \n",
    "    목표: max log P(D_i | z_i; θ)\n",
    "    - z_i가 문서 D_i를 재구성할 수 있도록 학습\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        lr_z: float = 1e-3,\n",
    "        lr_lora: float = 1e-4,\n",
    "        llm_frozen: bool = True,\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.llm_frozen = llm_frozen\n",
    "        \n",
    "        # Optimizer 구성\n",
    "        param_groups = [\n",
    "            {\"params\": [model.doc_vectors], \"lr\": lr_z, \"name\": \"z\"},\n",
    "            {\"params\": model.z_to_embedding.parameters(), \"lr\": lr_lora, \"name\": \"projection\"},\n",
    "        ]\n",
    "        \n",
    "        if not llm_frozen:\n",
    "            lora_params = [p for n, p in model.llm.named_parameters() if \"lora\" in n.lower()]\n",
    "            param_groups.append({\"params\": lora_params, \"lr\": lr_lora, \"name\": \"lora\"})\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(param_groups)\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        \"\"\"단일 training step\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        doc_ids = batch[\"doc_id\"].to(self.device)\n",
    "        input_ids = batch[\"input_ids\"].to(self.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "        \n",
    "        # Forward\n",
    "        loss = self.model.write_phase_forward(\n",
    "            doc_ids=doc_ids,\n",
    "            doc_input_ids=input_ids,\n",
    "            doc_attention_mask=attention_mask,\n",
    "        )\n",
    "        \n",
    "        # Backward\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def train_epoch(self, dataloader, epoch: int):\n",
    "        \"\"\"한 epoch 학습\"\"\"\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
    "        for batch in pbar:\n",
    "            loss = self.train_step(batch)\n",
    "            total_loss += loss\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({\"loss\": f\"{loss:.4f}\"})\n",
    "        \n",
    "        avg_loss = total_loss / max(num_batches, 1)\n",
    "        return avg_loss\n",
    "\n",
    "print(\"WritePhaseTrainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training 실행 (모델이 로드된 경우)\n",
    "# trainer = WritePhaseTrainer(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     lr_z=write_config.lr_z,\n",
    "#     lr_lora=write_config.lr_lora,\n",
    "#     llm_frozen=write_config.llm_frozen,\n",
    "#     device=device,\n",
    "# )\n",
    "\n",
    "# losses = []\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     avg_loss = trainer.train_epoch(write_loader, epoch + 1)\n",
    "#     losses.append(avg_loss)\n",
    "#     print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# losses가 있을 경우\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(losses)\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Write Phase Training Loss\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. z_i 품질 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reconstruction(model, corpus, tokenizer, device, num_samples=5):\n",
    "    \"\"\"\n",
    "    z_i가 문서를 얼마나 잘 재구성하는지 평가\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for doc_id in list(corpus.keys())[:num_samples]:\n",
    "        original = corpus[doc_id]\n",
    "        \n",
    "        # z_i로 생성\n",
    "        with torch.no_grad():\n",
    "            doc_ids = torch.tensor([doc_id]).to(device)\n",
    "            generated_ids = model.generate_from_z(\n",
    "                doc_ids=doc_ids,\n",
    "                max_new_tokens=100,\n",
    "            )\n",
    "            generated = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        results.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"original\": original[:200],\n",
    "            \"generated\": generated[:200],\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 평가 실행\n",
    "# results = evaluate_reconstruction(model, sample_corpus, tokenizer, device)\n",
    "# for r in results:\n",
    "#     print(f\"\\n=== Doc {r['doc_id']} ===\")\n",
    "#     print(f\"Original: {r['original']}\")\n",
    "#     print(f\"Generated: {r['generated']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Checkpoint 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, save_path):\n",
    "    \"\"\"체크포인트 저장\"\"\"\n",
    "    save_path = Path(save_path)\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"loss\": loss,\n",
    "        \"doc_vectors\": model.doc_vectors.data.cpu(),\n",
    "        \"z_to_embedding_state_dict\": model.z_to_embedding.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "    \n",
    "    # LoRA weights도 저장\n",
    "    model.llm.save_pretrained(save_path.parent / \"lora_weights\")\n",
    "    \n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Checkpoint saved to {save_path}\")\n",
    "\n",
    "# 저장\n",
    "# save_checkpoint(\n",
    "#     model, trainer.optimizer, NUM_EPOCHS, losses[-1],\n",
    "#     PROJECT_ROOT / \"checkpoints\" / \"phase1\" / \"write_final.pt\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, checkpoint_path):\n",
    "    \"\"\"체크포인트 로드\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    \n",
    "    # z vectors 로드\n",
    "    model.doc_vectors.data = checkpoint[\"doc_vectors\"].to(model.doc_vectors.device)\n",
    "    \n",
    "    # Projection 로드\n",
    "    model.z_to_embedding.load_state_dict(checkpoint[\"z_to_embedding_state_dict\"])\n",
    "    \n",
    "    print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}, loss: {checkpoint['loss']:.4f}\")\n",
    "    \n",
    "    return checkpoint\n",
    "\n",
    "# 로드\n",
    "# checkpoint = load_checkpoint(model, PROJECT_ROOT / \"checkpoints\" / \"phase1\" / \"write_final.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
