{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Analysis and Visualization\n",
    "\n",
    "실험 결과 분석 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\".\").resolve().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. z 벡터 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_z_vectors(doc_vectors, labels=None, method=\"tsne\", perplexity=30):\n",
    "    \"\"\"\n",
    "    z_i 벡터 2D 시각화\n",
    "    \n",
    "    Args:\n",
    "        doc_vectors: [num_docs, m_tokens, z_dim]\n",
    "        labels: 문서 카테고리 (optional)\n",
    "        method: 'tsne' or 'pca'\n",
    "    \"\"\"\n",
    "    # Mean pool across m_tokens\n",
    "    z_mean = doc_vectors.mean(dim=1).cpu().numpy()  # [num_docs, z_dim]\n",
    "    \n",
    "    # Dimensionality reduction\n",
    "    if method == \"tsne\":\n",
    "        reducer = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "    else:\n",
    "        reducer = PCA(n_components=2)\n",
    "    \n",
    "    z_2d = reducer.fit_transform(z_mean)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    if labels is not None:\n",
    "        unique_labels = list(set(labels))\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n",
    "        \n",
    "        for i, label in enumerate(unique_labels):\n",
    "            mask = [l == label for l in labels]\n",
    "            plt.scatter(\n",
    "                z_2d[mask, 0], z_2d[mask, 1],\n",
    "                c=[colors[i]], label=label, alpha=0.6, s=50\n",
    "            )\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.scatter(z_2d[:, 0], z_2d[:, 1], alpha=0.6, s=50)\n",
    "    \n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.title(f\"Document Vectors ({method.upper()})\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    return z_2d\n",
    "\n",
    "# 예시 데이터\n",
    "# doc_vectors = model.doc_vectors.data\n",
    "# z_2d = visualize_z_vectors(doc_vectors, method=\"tsne\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 시각화 (랜덤 데이터)\n",
    "np.random.seed(42)\n",
    "sample_vectors = torch.randn(100, 4, 256)\n",
    "sample_labels = [\"Category A\"] * 30 + [\"Category B\"] * 40 + [\"Category C\"] * 30\n",
    "\n",
    "z_2d = visualize_z_vectors(sample_vectors, sample_labels, method=\"pca\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Selection 분포 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_selection_distribution(selection_scores, gold_doc_ids):\n",
    "    \"\"\"\n",
    "    Selection score 분포 분석\n",
    "    \n",
    "    Args:\n",
    "        selection_scores: [num_queries, num_docs]\n",
    "        gold_doc_ids: list of gold document indices per query\n",
    "    \"\"\"\n",
    "    gold_scores = []\n",
    "    non_gold_scores = []\n",
    "    \n",
    "    for i, scores in enumerate(selection_scores):\n",
    "        gold_ids = gold_doc_ids[i]\n",
    "        for j, score in enumerate(scores):\n",
    "            if j in gold_ids:\n",
    "                gold_scores.append(score)\n",
    "            else:\n",
    "                non_gold_scores.append(score)\n",
    "    \n",
    "    # 분포 시각화\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.hist(non_gold_scores, bins=50, alpha=0.5, label=\"Non-gold\", color=\"blue\", density=True)\n",
    "    plt.hist(gold_scores, bins=50, alpha=0.5, label=\"Gold\", color=\"red\", density=True)\n",
    "    \n",
    "    plt.xlabel(\"Selection Score\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(\"Selection Score Distribution: Gold vs Non-gold Documents\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 통계\n",
    "    print(f\"Gold scores - Mean: {np.mean(gold_scores):.4f}, Std: {np.std(gold_scores):.4f}\")\n",
    "    print(f\"Non-gold scores - Mean: {np.mean(non_gold_scores):.4f}, Std: {np.std(non_gold_scores):.4f}\")\n",
    "    \n",
    "    return gold_scores, non_gold_scores\n",
    "\n",
    "# 예시\n",
    "sample_scores = np.random.randn(50, 100)\n",
    "sample_gold = [[np.random.randint(0, 100)] for _ in range(50)]\n",
    "# analyze_selection_distribution(sample_scores, sample_gold)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.error_analysis import analyze_errors, analyze_multihop_errors\n",
    "\n",
    "# Error categories\n",
    "error_categories = {\n",
    "    \"selection_failure\": \"Gold doc이 top-k에 없음\",\n",
    "    \"generation_failure\": \"Selection은 맞지만 답이 틀림\",\n",
    "    \"both_failure\": \"Selection과 Generation 모두 실패\",\n",
    "    \"success\": \"정답\",\n",
    "}\n",
    "\n",
    "for cat, desc in error_categories.items():\n",
    "    print(f\"{cat}: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 error 분포\n",
    "error_stats = {\n",
    "    \"success\": 45.2,\n",
    "    \"selection_failure\": 25.3,\n",
    "    \"generation_failure\": 18.5,\n",
    "    \"both_failure\": 11.0,\n",
    "}\n",
    "\n",
    "# Pie chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = [\"#2ecc71\", \"#e74c3c\", \"#3498db\", \"#9b59b6\"]\n",
    "explode = (0.05, 0, 0, 0)\n",
    "\n",
    "plt.pie(\n",
    "    error_stats.values(),\n",
    "    labels=error_stats.keys(),\n",
    "    autopct=\"%1.1f%%\",\n",
    "    colors=colors,\n",
    "    explode=explode,\n",
    "    shadow=True,\n",
    ")\n",
    "plt.title(\"Error Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-hop Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-hop 전용 error 분류\n",
    "multihop_errors = {\n",
    "    \"bridge_not_found\": 35.2,\n",
    "    \"bridge_not_recognized\": 22.1,\n",
    "    \"propagation_error\": 15.3,\n",
    "    \"success\": 27.4,\n",
    "}\n",
    "\n",
    "# Bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = [\"#e74c3c\", \"#f39c12\", \"#9b59b6\", \"#2ecc71\"]\n",
    "bars = plt.bar(multihop_errors.keys(), multihop_errors.values(), color=colors)\n",
    "\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.title(\"Multi-hop Error Analysis (HotpotQA)\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# 값 표시\n",
    "for bar, val in zip(bars, multihop_errors.values()):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "             f\"{val}%\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attention Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_heatmap(attention_weights, query_tokens, doc_ids):\n",
    "    \"\"\"\n",
    "    Query-Document attention heatmap\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: [query_len, num_docs]\n",
    "        query_tokens: list of query tokens\n",
    "        doc_ids: list of document IDs\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        attention_weights,\n",
    "        xticklabels=doc_ids,\n",
    "        yticklabels=query_tokens,\n",
    "        cmap=\"YlOrRd\",\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "    )\n",
    "    \n",
    "    plt.xlabel(\"Document ID\")\n",
    "    plt.ylabel(\"Query Token\")\n",
    "    plt.title(\"Query-Document Attention Weights\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "# 예시\n",
    "sample_attn = np.random.rand(8, 5)\n",
    "sample_attn = sample_attn / sample_attn.sum(axis=1, keepdims=True)  # normalize\n",
    "query_tokens = [\"What\", \"is\", \"the\", \"capital\", \"of\", \"France\", \"?\", \"[PAD]\"]\n",
    "doc_ids = [\"Doc0\", \"Doc1\", \"Doc2\", \"Doc3\", \"Doc4\"]\n",
    "\n",
    "plot_attention_heatmap(sample_attn, query_tokens, doc_ids)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(history: dict):\n",
    "    \"\"\"\n",
    "    학습 곡선 시각화\n",
    "    \n",
    "    Args:\n",
    "        history: {\"loss\": [...], \"em\": [...], \"f1\": [...]}\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss\n",
    "    ax1 = axes[0]\n",
    "    if \"train_loss\" in history:\n",
    "        ax1.plot(history[\"train_loss\"], label=\"Train\", color=\"#3498db\")\n",
    "    if \"val_loss\" in history:\n",
    "        ax1.plot(history[\"val_loss\"], label=\"Validation\", color=\"#e74c3c\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(\"Training Loss\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Metrics\n",
    "    ax2 = axes[1]\n",
    "    if \"em\" in history:\n",
    "        ax2.plot(history[\"em\"], label=\"EM\", color=\"#2ecc71\")\n",
    "    if \"f1\" in history:\n",
    "        ax2.plot(history[\"f1\"], label=\"F1\", color=\"#9b59b6\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Score (%)\")\n",
    "    ax2.set_title(\"Evaluation Metrics\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "# 예시 데이터\n",
    "sample_history = {\n",
    "    \"train_loss\": [2.5, 1.8, 1.4, 1.1, 0.9, 0.75, 0.65, 0.58, 0.52, 0.48],\n",
    "    \"val_loss\": [2.6, 1.9, 1.5, 1.2, 1.0, 0.85, 0.78, 0.72, 0.68, 0.66],\n",
    "    \"em\": [15, 25, 32, 38, 42, 44, 45.5, 46.2, 46.5, 46.8],\n",
    "    \"f1\": [22, 32, 40, 46, 50, 52, 53.5, 54.2, 54.5, 54.8],\n",
    "}\n",
    "\n",
    "plot_training_curves(sample_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Corpus Size Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus size vs Performance/Efficiency\n",
    "scaling_data = {\n",
    "    \"corpus_size\": [1000, 5000, 10000, 50000, 100000],\n",
    "    \"EM\": [42.5, 44.2, 45.8, 46.5, 46.8],\n",
    "    \"selection_latency_ms\": [2.1, 3.5, 5.2, 15.8, 32.4],\n",
    "    \"storage_mb\": [4, 20, 40, 200, 400],\n",
    "}\n",
    "\n",
    "df_scaling = pd.DataFrame(scaling_data)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# EM\n",
    "ax1 = axes[0]\n",
    "ax1.semilogx(df_scaling[\"corpus_size\"], df_scaling[\"EM\"], \"o-\", color=\"#3498db\")\n",
    "ax1.set_xlabel(\"Corpus Size\")\n",
    "ax1.set_ylabel(\"EM (%)\")\n",
    "ax1.set_title(\"Performance Scaling\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Latency\n",
    "ax2 = axes[1]\n",
    "ax2.semilogx(df_scaling[\"corpus_size\"], df_scaling[\"selection_latency_ms\"], \"o-\", color=\"#e74c3c\")\n",
    "ax2.set_xlabel(\"Corpus Size\")\n",
    "ax2.set_ylabel(\"Selection Latency (ms)\")\n",
    "ax2.set_title(\"Latency Scaling\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Storage\n",
    "ax3 = axes[2]\n",
    "ax3.loglog(df_scaling[\"corpus_size\"], df_scaling[\"storage_mb\"], \"o-\", color=\"#2ecc71\")\n",
    "ax3.set_xlabel(\"Corpus Size\")\n",
    "ax3.set_ylabel(\"Storage (MB)\")\n",
    "ax3.set_title(\"Storage Scaling\")\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_analysis_results(results: dict, figures_dir: Path):\n",
    "    \"\"\"\n",
    "    분석 결과 저장\n",
    "    \"\"\"\n",
    "    figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # JSON으로 수치 결과 저장\n",
    "    with open(figures_dir / \"analysis_results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {figures_dir}\")\n",
    "\n",
    "# 저장\n",
    "# results = {\n",
    "#     \"error_distribution\": error_stats,\n",
    "#     \"multihop_errors\": multihop_errors,\n",
    "#     \"scaling\": scaling_data,\n",
    "# }\n",
    "# save_analysis_results(results, PROJECT_ROOT / \"results\" / \"analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
