# Phase 2: Ablation Studies
# 목표: 최적 하이퍼파라미터 탐색

model:
  llm_name: "Qwen/Qwen3-8B"
  quantization: "4bit"
  lora:
    r: 64
    alpha: 128
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
    dropout: 0.05

parametric_qa:
  num_docs: 10000
  z_dim: 256  # will be overridden per ablation
  m_tokens: 8  # will be overridden per ablation
  selection_method: "cosine"  # will be overridden per ablation

data:
  dataset: "natural_questions"
  corpus_size: 10000
  train_samples: 5000
  eval_samples: 1000
  max_doc_length: 512
  max_query_length: 128

ablations:
  z_dim:
    values: [64, 128, 256, 512, 1024]
    metric: ["em", "reconstruction_ppl", "storage_mb", "latency_ms"]

  m_tokens:
    values: [1, 4, 8, 16, 32]
    metric: ["em", "rouge_l", "context_budget"]

  selection_method:
    values: ["cosine", "learned_router", "attention"]
    metric: ["recall_at_5", "selection_precision", "latency_ms"]

  lora_rank:
    values: [8, 32, 64]
    settings:
      - {r: 8, alpha: 16, llm_frozen: false}
      - {r: 32, alpha: 64, llm_frozen: false}
      - {r: 64, alpha: 128, llm_frozen: false}
    include_frozen: true  # also test llm_frozen=true as baseline
    metric: ["em", "vram_gb", "training_speed"]

  top_k:
    values: [1, 3, 5, 10]
    metric: ["em", "f1", "latency_ms"]

write_phase:
  epochs: 100
  lr: 1e-3
  llm_frozen: true
  batch_size: 4

read_phase:
  epochs: 3
  lr_llm: 2e-5
  lr_z: 1e-3
  lr_proj: 1e-4
  alpha_retrieval: 0.1
  batch_size: 2
  gradient_accumulation: 8
  warmup_ratio: 0.1

evaluation:
  metrics: ["em", "f1", "recall_at_k", "perplexity", "latency", "memory", "storage"]
  seeds: [42]  # single seed for ablation speed

hardware:
  device: "cuda"
  fp16: true
  gradient_checkpointing: true

logging:
  project: "parametric-qa"
  run_name: "phase2-ablation"
  use_wandb: true
  log_dir: "./logs/phase2"
  save_dir: "./checkpoints/phase2"
