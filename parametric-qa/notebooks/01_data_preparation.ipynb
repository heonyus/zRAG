{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Data Preparation\n",
    "\n",
    "데이터 다운로드, 전처리, DataLoader 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\".\").resolve().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터셋 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Natural Questions 다운로드\n",
    "DATASET_NAME = \"natural_questions\"  # or \"hotpot_qa\", \"trivia_qa\"\n",
    "SAVE_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Downloading {DATASET_NAME}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_natural_questions():\n",
    "    \"\"\"Natural Questions 데이터셋 다운로드\"\"\"\n",
    "    # NQ는 매우 크므로 validation split만 사용 (POC용)\n",
    "    dataset = load_dataset(\"natural_questions\", \"default\", split=\"validation\")\n",
    "    \n",
    "    # 간단한 전처리\n",
    "    processed = []\n",
    "    for item in dataset:\n",
    "        question = item[\"question\"][\"text\"]\n",
    "        \n",
    "        # Short answer 추출\n",
    "        annotations = item[\"annotations\"]\n",
    "        if annotations[\"short_answers\"][0]:\n",
    "            start = annotations[\"short_answers\"][0][\"start_token\"]\n",
    "            end = annotations[\"short_answers\"][0][\"end_token\"]\n",
    "            doc_tokens = item[\"document\"][\"tokens\"][\"token\"]\n",
    "            answer = \" \".join(doc_tokens[start:end])\n",
    "            \n",
    "            processed.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"document\": \" \".join(doc_tokens[:512]),  # 앞부분만\n",
    "            })\n",
    "    \n",
    "    return processed\n",
    "\n",
    "# 다운로드 (시간이 오래 걸릴 수 있음)\n",
    "# nq_data = download_natural_questions()\n",
    "# print(f\"Downloaded {len(nq_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hotpot_qa():\n",
    "    \"\"\"HotpotQA 데이터셋 다운로드\"\"\"\n",
    "    dataset = load_dataset(\"hotpot_qa\", \"fullwiki\", split=\"validation\")\n",
    "    \n",
    "    processed = []\n",
    "    for item in dataset:\n",
    "        processed.append({\n",
    "            \"question\": item[\"question\"],\n",
    "            \"answer\": item[\"answer\"],\n",
    "            \"supporting_facts\": item[\"supporting_facts\"],\n",
    "            \"context\": item[\"context\"],\n",
    "            \"type\": item[\"type\"],  # bridge or comparison\n",
    "            \"level\": item[\"level\"],\n",
    "        })\n",
    "    \n",
    "    return processed\n",
    "\n",
    "# hotpot_data = download_hotpot_qa()\n",
    "# print(f\"Downloaded {len(hotpot_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoTokenizer\n\n# Tokenizer 로드\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"Vocab size: {tokenizer.vocab_size}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_parametric_qa(\n",
    "    raw_data: list,\n",
    "    tokenizer,\n",
    "    max_doc_length: int = 512,\n",
    "    max_query_length: int = 128,\n",
    "    max_answer_length: int = 64,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parametric QA를 위한 전처리\n",
    "    \n",
    "    Returns:\n",
    "        corpus: {doc_id: document_text}\n",
    "        qa_pairs: [{question, answer, gold_doc_ids}]\n",
    "    \"\"\"\n",
    "    corpus = {}\n",
    "    qa_pairs = []\n",
    "    doc_id = 0\n",
    "    \n",
    "    for item in raw_data:\n",
    "        # Document 저장\n",
    "        doc_text = item.get(\"document\", \"\")\n",
    "        if doc_text:\n",
    "            corpus[doc_id] = doc_text[:2000]  # 길이 제한\n",
    "            \n",
    "            # QA pair\n",
    "            qa_pairs.append({\n",
    "                \"question\": item[\"question\"],\n",
    "                \"answer\": item[\"answer\"],\n",
    "                \"gold_doc_ids\": [doc_id],\n",
    "            })\n",
    "            \n",
    "            doc_id += 1\n",
    "    \n",
    "    return corpus, qa_pairs\n",
    "\n",
    "# 예시 데이터로 테스트\n",
    "sample_data = [\n",
    "    {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"answer\": \"Paris\",\n",
    "        \"document\": \"France is a country in Western Europe. Paris is the capital and largest city of France.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who wrote Romeo and Juliet?\",\n",
    "        \"answer\": \"William Shakespeare\",\n",
    "        \"document\": \"Romeo and Juliet is a tragedy written by William Shakespeare early in his career.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "corpus, qa_pairs = preprocess_for_parametric_qa(sample_data, tokenizer)\n",
    "print(f\"Corpus size: {len(corpus)}\")\n",
    "print(f\"QA pairs: {len(qa_pairs)}\")\n",
    "print(f\"Sample: {qa_pairs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DataLoader 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class WritePhaseDataset(Dataset):\n",
    "    \"\"\"Write Phase용 Dataset: Document reconstruction\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus: dict, tokenizer, max_length: int = 512):\n",
    "        self.corpus = corpus\n",
    "        self.doc_ids = list(corpus.keys())\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.doc_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        doc_id = self.doc_ids[idx]\n",
    "        doc_text = self.corpus[doc_id]\n",
    "        \n",
    "        encoded = self.tokenizer(\n",
    "            doc_text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"doc_id\": doc_id,\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "# 테스트\n",
    "write_dataset = WritePhaseDataset(corpus, tokenizer)\n",
    "print(f\"Write dataset size: {len(write_dataset)}\")\n",
    "print(f\"Sample: {write_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadPhaseDataset(Dataset):\n",
    "    \"\"\"Read Phase용 Dataset: QA generation\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        qa_pairs: list,\n",
    "        tokenizer,\n",
    "        max_query_length: int = 128,\n",
    "        max_answer_length: int = 64,\n",
    "    ):\n",
    "        self.qa_pairs = qa_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_query_length = max_query_length\n",
    "        self.max_answer_length = max_answer_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.qa_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.qa_pairs[idx]\n",
    "        \n",
    "        # Query encoding\n",
    "        query_encoded = self.tokenizer(\n",
    "            item[\"question\"],\n",
    "            max_length=self.max_query_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # Answer encoding\n",
    "        answer_encoded = self.tokenizer(\n",
    "            item[\"answer\"],\n",
    "            max_length=self.max_answer_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"query_ids\": query_encoded[\"input_ids\"].squeeze(0),\n",
    "            \"query_mask\": query_encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"answer_ids\": answer_encoded[\"input_ids\"].squeeze(0),\n",
    "            \"answer_mask\": answer_encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"gold_doc_ids\": torch.tensor(item[\"gold_doc_ids\"]),\n",
    "        }\n",
    "\n",
    "# 테스트\n",
    "read_dataset = ReadPhaseDataset(qa_pairs, tokenizer)\n",
    "print(f\"Read dataset size: {len(read_dataset)}\")\n",
    "print(f\"Sample keys: {read_dataset[0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "write_loader = DataLoader(\n",
    "    write_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "read_loader = DataLoader(\n",
    "    read_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# 배치 확인\n",
    "for batch in write_loader:\n",
    "    print(\"Write batch:\")\n",
    "    print(f\"  doc_id: {batch['doc_id']}\")\n",
    "    print(f\"  input_ids shape: {batch['input_ids'].shape}\")\n",
    "    break\n",
    "\n",
    "for batch in read_loader:\n",
    "    print(\"\\nRead batch:\")\n",
    "    print(f\"  query_ids shape: {batch['query_ids'].shape}\")\n",
    "    print(f\"  answer_ids shape: {batch['answer_ids'].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 전처리된 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_processed_data(corpus, qa_pairs, save_dir):\n",
    "    \"\"\"전처리된 데이터 저장\"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Corpus 저장\n",
    "    with open(save_dir / \"corpus.json\", \"w\") as f:\n",
    "        json.dump(corpus, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # QA pairs 저장\n",
    "    with open(save_dir / \"qa_pairs.json\", \"w\") as f:\n",
    "        json.dump(qa_pairs, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Saved to {save_dir}\")\n",
    "\n",
    "# 저장\n",
    "# save_processed_data(corpus, qa_pairs, PROJECT_ROOT / \"data\" / \"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(load_dir):\n",
    "    \"\"\"전처리된 데이터 로드\"\"\"\n",
    "    load_dir = Path(load_dir)\n",
    "    \n",
    "    with open(load_dir / \"corpus.json\") as f:\n",
    "        corpus = json.load(f)\n",
    "    \n",
    "    with open(load_dir / \"qa_pairs.json\") as f:\n",
    "        qa_pairs = json.load(f)\n",
    "    \n",
    "    # corpus keys를 int로 변환\n",
    "    corpus = {int(k): v for k, v in corpus.items()}\n",
    "    \n",
    "    return corpus, qa_pairs\n",
    "\n",
    "# 로드\n",
    "# corpus, qa_pairs = load_processed_data(PROJECT_ROOT / \"data\" / \"processed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}