{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Model Architecture Exploration\n",
    "\n",
    "Parametric QA 모델 구조 이해 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\".\").resolve().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document Vector (z_i) 개념"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametric QA의 핵심: Document → Learnable Vector z_i\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_DOCS = 1000      # 문서 수\n",
    "M_TOKENS = 4         # z_i가 차지하는 virtual token 수\n",
    "Z_DIM = 256          # z_i의 차원\n",
    "\n",
    "# Document Vectors: [num_docs, m_tokens, z_dim]\n",
    "doc_vectors = nn.Parameter(torch.randn(NUM_DOCS, M_TOKENS, Z_DIM) * 0.02)\n",
    "\n",
    "print(f\"Document vectors shape: {doc_vectors.shape}\")\n",
    "print(f\"Total parameters: {doc_vectors.numel():,}\")\n",
    "print(f\"Storage per doc: {M_TOKENS * Z_DIM * 4} bytes (float32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_i 인덱싱\n",
    "doc_indices = torch.tensor([0, 5, 10])  # 3개 문서 선택\n",
    "selected_z = doc_vectors[doc_indices]   # [3, m_tokens, z_dim]\n",
    "\n",
    "print(f\"Selected z shape: {selected_z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Query Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E5-base-v2를 query encoder로 사용\n",
    "QUERY_ENCODER_NAME = \"intfloat/e5-base-v2\"\n",
    "\n",
    "# 로드 (실제로는 시간이 걸릴 수 있음)\n",
    "# query_encoder = AutoModel.from_pretrained(QUERY_ENCODER_NAME)\n",
    "# query_tokenizer = AutoTokenizer.from_pretrained(QUERY_ENCODER_NAME)\n",
    "\n",
    "print(f\"Query encoder: {QUERY_ENCODER_NAME}\")\n",
    "print(\"E5 output dim: 768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query embedding 추출 (mean pooling)\n",
    "def get_query_embedding(query_encoder, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Query를 embedding으로 변환\n",
    "    \"\"\"\n",
    "    outputs = query_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    hidden_states = outputs.last_hidden_state  # [B, seq_len, hidden]\n",
    "    \n",
    "    # Mean pooling\n",
    "    mask_expanded = attention_mask.unsqueeze(-1).float()\n",
    "    sum_hidden = (hidden_states * mask_expanded).sum(dim=1)\n",
    "    sum_mask = mask_expanded.sum(dim=1).clamp(min=1e-9)\n",
    "    query_emb = sum_hidden / sum_mask  # [B, hidden]\n",
    "    \n",
    "    return query_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Selection Methods (Router)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.router import CosineSelector, LearnedRouter, AttentionSelector\n",
    "\n",
    "# 세 가지 selection method\n",
    "print(\"Available selection methods:\")\n",
    "print(\"1. CosineSelector: cosine similarity 기반\")\n",
    "print(\"2. LearnedRouter: 학습 가능한 MLP router\")\n",
    "print(\"3. AttentionSelector: cross-attention 기반\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CosineSelector 예시\n",
    "class CosineSelector(nn.Module):\n",
    "    def __init__(self, query_dim: int, z_dim: int, temperature: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.query_proj = nn.Linear(query_dim, z_dim)\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, query_emb, doc_vectors):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query_emb: [B, query_dim]\n",
    "            doc_vectors: [num_docs, m_tokens, z_dim]\n",
    "        Returns:\n",
    "            scores: [B, num_docs]\n",
    "        \"\"\"\n",
    "        # Project query\n",
    "        q = self.query_proj(query_emb)  # [B, z_dim]\n",
    "        q = nn.functional.normalize(q, dim=-1)\n",
    "        \n",
    "        # Mean pool z_i across m_tokens\n",
    "        z_mean = doc_vectors.mean(dim=1)  # [num_docs, z_dim]\n",
    "        z_mean = nn.functional.normalize(z_mean, dim=-1)\n",
    "        \n",
    "        # Cosine similarity\n",
    "        scores = torch.matmul(q, z_mean.T) / self.temperature  # [B, num_docs]\n",
    "        \n",
    "        return scores\n",
    "\n",
    "# 테스트\n",
    "selector = CosineSelector(query_dim=768, z_dim=Z_DIM)\n",
    "dummy_query = torch.randn(2, 768)  # batch of 2 queries\n",
    "scores = selector(dummy_query, doc_vectors)\n",
    "print(f\"Scores shape: {scores.shape}\")  # [2, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LLM with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# QLoRA 설정\nLLM_NAME = \"Qwen/Qwen3-8B\"\n\n# 4-bit quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# LoRA config\nlora_config = LoraConfig(\n    r=16,                      # rank\n    lora_alpha=32,             # scaling\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nprint(\"QLoRA configuration:\")\nprint(f\"  Quantization: 4-bit NF4\")\nprint(f\"  LoRA rank: {lora_config.r}\")\nprint(f\"  Target modules: {lora_config.target_modules}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 로드 (메모리가 충분할 때만 실행)\n",
    "# llm = AutoModelForCausalLM.from_pretrained(\n",
    "#     LLM_NAME,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "# llm = get_peft_model(llm, lora_config)\n",
    "# llm.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. z → LLM Embedding Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# z_i를 LLM embedding 공간으로 projection\nLLM_HIDDEN_DIM = 4096  # Qwen3-8B hidden size\n\nz_to_embedding = nn.Sequential(\n    nn.Linear(Z_DIM, LLM_HIDDEN_DIM),\n    nn.GELU(),\n    nn.Linear(LLM_HIDDEN_DIM, LLM_HIDDEN_DIM),\n    nn.LayerNorm(LLM_HIDDEN_DIM),\n)\n\n# 테스트\nz_sample = doc_vectors[0:1]  # [1, m_tokens, z_dim]\nz_projected = z_to_embedding(z_sample)  # [1, m_tokens, llm_hidden_dim]\nprint(f\"Projected z shape: {z_projected.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Full Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ParametricQA 모델 구조 개요\n\nclass ParametricQAOverview(nn.Module):\n    \"\"\"\n    Parametric QA 모델 구조 (간소화 버전)\n    \n    Components:\n    1. doc_vectors: [num_docs, m_tokens, z_dim] - learnable document vectors\n    2. query_encoder: E5-base-v2 - query embedding\n    3. selector: CosineSelector/LearnedRouter/AttentionSelector\n    4. z_to_embedding: z_i → LLM embedding space\n    5. llm: Qwen3-8B with QLoRA\n    \n    Forward pass:\n    - Write phase: z_i → LLM → reconstruct D_i\n    - Read phase: query → select z_i → z_i + query → LLM → answer\n    \"\"\"\n    \n    def __init__(self, num_docs, z_dim, m_tokens):\n        super().__init__()\n        self.doc_vectors = nn.Parameter(torch.randn(num_docs, m_tokens, z_dim) * 0.02)\n        # ... other components\n    \n    def write_phase_forward(self, doc_ids, doc_input_ids, doc_attention_mask):\n        \"\"\"Write phase: z_i → reconstruct D_i\"\"\"\n        z = self.doc_vectors[doc_ids]  # [B, m_tokens, z_dim]\n        # Project and generate\n        pass\n    \n    def forward(self, query_ids, doc_indices, answer_ids):\n        \"\"\"Read phase: query + z_selected → answer\"\"\"\n        # Select documents\n        z_selected = self.doc_vectors[doc_indices]  # [B, k, m_tokens, z_dim]\n        # Generate answer\n        pass\n\nprint(\"Model architecture loaded (see docstring for details)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 실제 모델 로드 (전체 모듈)\nfrom models.parametric_qa import ParametricQA\n\n# Config\nmodel_config = {\n    \"llm_name\": \"Qwen/Qwen3-8B\",\n    \"num_docs\": 1000,\n    \"z_dim\": 256,\n    \"m_tokens\": 4,\n    \"selection_method\": \"cosine\",\n    \"query_encoder_name\": \"intfloat/e5-base-v2\",\n    \"lora_r\": 16,\n    \"lora_alpha\": 32,\n    \"use_4bit\": True,\n}\n\nprint(\"Model configuration:\")\nfor k, v in model_config.items():\n    print(f\"  {k}: {v}\")\n\n# 실제 초기화는 GPU 메모리가 충분할 때만\n# model = ParametricQA(**model_config)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Parameter Count 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def count_parameters(model):\n    \"\"\"모델의 파라미터 수 계산\"\"\"\n    total = sum(p.numel() for p in model.parameters())\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return total, trainable\n\n# 예상 파라미터 수\nprint(\"Expected parameter counts:\")\nprint(f\"  doc_vectors: {NUM_DOCS * M_TOKENS * Z_DIM:,}\")\nprint(f\"  query_encoder (E5-base): ~110M (frozen)\")\nprint(f\"  z_to_embedding: ~{2 * Z_DIM * LLM_HIDDEN_DIM:,}\")\nprint(f\"  LLM (Qwen3-8B): ~8B (mostly frozen, QLoRA ~8M trainable)\")\nprint(f\"  selector: ~{768 * Z_DIM:,}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}