{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Evaluation\n",
    "\n",
    "QA 성능, Write 품질, Efficiency 종합 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\".\").resolve().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Metrics 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.metrics import (\n",
    "    compute_em,\n",
    "    compute_f1,\n",
    "    compute_recall_at_k,\n",
    "    compute_mrr,\n",
    "    compute_rouge_l,\n",
    ")\n",
    "\n",
    "# Metric 테스트\n",
    "prediction = \"Paris is the capital\"\n",
    "reference = \"Paris\"\n",
    "\n",
    "print(f\"EM: {compute_em(prediction, reference)}\")\n",
    "print(f\"F1: {compute_f1(prediction, reference):.4f}\")\n",
    "print(f\"ROUGE-L: {compute_rouge_l(prediction, reference):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval metrics 테스트\n",
    "retrieved = [3, 1, 5, 2, 0]\n",
    "gold = [1, 5]\n",
    "\n",
    "print(f\"Recall@5: {compute_recall_at_k(retrieved, gold, k=5):.4f}\")\n",
    "print(f\"MRR: {compute_mrr(retrieved, gold):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. QA 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluate_qa import QAEvaluator\n",
    "\n",
    "# 샘플 데이터\n",
    "sample_qa_pairs = [\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\", \"gold_doc_ids\": [0]},\n",
    "    {\"question\": \"Who wrote Romeo and Juliet?\", \"answer\": \"William Shakespeare\", \"gold_doc_ids\": [1]},\n",
    "    {\"question\": \"What did Einstein develop?\", \"answer\": \"theory of relativity\", \"gold_doc_ids\": [3]},\n",
    "]\n",
    "\n",
    "print(f\"Sample QA pairs: {len(sample_qa_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA 평가 실행 (모델이 로드된 경우)\n",
    "# evaluator = QAEvaluator(model, tokenizer, device=\"cuda\")\n",
    "# results = evaluator.evaluate(sample_qa_pairs, top_k=5)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write Phase 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluate_write import WriteEvaluator\n",
    "\n",
    "# Write 평가 메트릭:\n",
    "# - Reconstruction loss (perplexity)\n",
    "# - BLEU / ROUGE 기반 재구성 품질\n",
    "# - z 벡터 clustering 품질 (optional)\n",
    "\n",
    "print(\"Write evaluation metrics:\")\n",
    "print(\"  - Reconstruction loss (perplexity)\")\n",
    "print(\"  - BLEU score\")\n",
    "print(\"  - ROUGE-L score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write 평가 실행\n",
    "# write_evaluator = WriteEvaluator(model, tokenizer, device=\"cuda\")\n",
    "# write_results = write_evaluator.evaluate(corpus, num_samples=100)\n",
    "# print(write_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Efficiency 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluate_efficiency import EfficiencyEvaluator\n",
    "\n",
    "# Efficiency 메트릭:\n",
    "# - Selection latency\n",
    "# - Generation latency\n",
    "# - Total latency\n",
    "# - Storage (z vectors)\n",
    "# - Peak memory\n",
    "\n",
    "print(\"Efficiency metrics:\")\n",
    "print(\"  - Selection latency (ms)\")\n",
    "print(\"  - Generation latency (ms)\")\n",
    "print(\"  - Total latency (ms)\")\n",
    "print(\"  - Storage per doc (bytes)\")\n",
    "print(\"  - Peak GPU memory (GB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency(model, tokenizer, question, device=\"cuda\", warmup=3, trials=10):\n",
    "    \"\"\"\n",
    "    Latency 측정\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize\n",
    "    encoded = tokenizer(\n",
    "        question, max_length=128, truncation=True,\n",
    "        padding=\"max_length\", return_tensors=\"pt\"\n",
    "    )\n",
    "    query_ids = encoded[\"input_ids\"].to(device)\n",
    "    query_mask = encoded[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        with torch.no_grad():\n",
    "            selected_ids, _ = model.select_documents(query_ids, query_mask, k=5)\n",
    "            _ = model.generate(\n",
    "                query_ids=query_ids,\n",
    "                doc_indices=selected_ids,\n",
    "                query_attention_mask=query_mask,\n",
    "                max_new_tokens=32,\n",
    "            )\n",
    "    \n",
    "    # CUDA sync\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Measure\n",
    "    selection_times = []\n",
    "    generation_times = []\n",
    "    \n",
    "    for _ in range(trials):\n",
    "        # Selection\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            selected_ids, _ = model.select_documents(query_ids, query_mask, k=5)\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        selection_times.append((time.perf_counter() - start) * 1000)\n",
    "        \n",
    "        # Generation\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(\n",
    "                query_ids=query_ids,\n",
    "                doc_indices=selected_ids,\n",
    "                query_attention_mask=query_mask,\n",
    "                max_new_tokens=32,\n",
    "            )\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        generation_times.append((time.perf_counter() - start) * 1000)\n",
    "    \n",
    "    return {\n",
    "        \"selection_latency_ms\": sum(selection_times) / len(selection_times),\n",
    "        \"generation_latency_ms\": sum(generation_times) / len(generation_times),\n",
    "        \"total_latency_ms\": sum(selection_times) / len(selection_times) + sum(generation_times) / len(generation_times),\n",
    "    }\n",
    "\n",
    "# 실행\n",
    "# latency = measure_latency(model, tokenizer, \"What is the capital of France?\")\n",
    "# print(latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_storage(model, num_docs):\n",
    "    \"\"\"\n",
    "    Storage 측정\n",
    "    \"\"\"\n",
    "    z_dim = model.doc_vectors.shape[2]\n",
    "    m_tokens = model.doc_vectors.shape[1]\n",
    "    \n",
    "    bytes_per_doc = m_tokens * z_dim * 4  # float32\n",
    "    total_bytes = num_docs * bytes_per_doc\n",
    "    \n",
    "    return {\n",
    "        \"bytes_per_doc\": bytes_per_doc,\n",
    "        \"total_storage_mb\": total_bytes / (1024 * 1024),\n",
    "        \"z_dim\": z_dim,\n",
    "        \"m_tokens\": m_tokens,\n",
    "    }\n",
    "\n",
    "# 예상 storage\n",
    "z_dim = 256\n",
    "m_tokens = 4\n",
    "num_docs = 10000\n",
    "bytes_per_doc = m_tokens * z_dim * 4\n",
    "print(f\"Storage per doc: {bytes_per_doc} bytes\")\n",
    "print(f\"Total for {num_docs:,} docs: {num_docs * bytes_per_doc / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAGAS 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS 메트릭\n",
    "# - Faithfulness: 생성된 답이 context에 충실한가\n",
    "# - Answer Relevancy: 답이 질문과 관련 있는가\n",
    "# - Context Relevancy: 검색된 context가 질문과 관련 있는가\n",
    "\n",
    "print(\"RAGAS metrics:\")\n",
    "print(\"  - Faithfulness\")\n",
    "print(\"  - Answer Relevancy\")\n",
    "print(\"  - Context Relevancy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS 평가는 LLM-as-judge 방식이므로 추가 API 필요\n",
    "# from evaluation.evaluate_ragas import RagasEvaluator\n",
    "\n",
    "# ragas_evaluator = RagasEvaluator(judge_model=\"gpt-4\")\n",
    "# ragas_results = ragas_evaluator.evaluate(predictions)\n",
    "# print(ragas_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 결과 데이터\n",
    "example_results = {\n",
    "    \"Parametric-QA\": {\"EM\": 45.2, \"F1\": 52.3, \"Recall@5\": 78.5},\n",
    "    \"No Retrieval\": {\"EM\": 22.1, \"F1\": 28.4, \"Recall@5\": 0},\n",
    "    \"BM25-RAG\": {\"EM\": 35.6, \"F1\": 42.1, \"Recall@5\": 65.2},\n",
    "    \"Dense-RAG\": {\"EM\": 40.3, \"F1\": 48.7, \"Recall@5\": 72.4},\n",
    "}\n",
    "\n",
    "# DataFrame 변환\n",
    "df = pd.DataFrame(example_results).T\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics = [\"EM\", \"F1\", \"Recall@5\"]\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    df[metric].plot(kind=\"bar\", ax=ax, color=[\"#4CAF50\", \"#2196F3\", \"#FF9800\", \"#9C27B0\"])\n",
    "    ax.set_title(metric)\n",
    "    ax.set_ylabel(\"Score (%)\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency comparison\n",
    "latency_data = {\n",
    "    \"Parametric-QA\": {\"Selection\": 5.2, \"Generation\": 120.5},\n",
    "    \"BM25-RAG\": {\"Selection\": 15.3, \"Generation\": 145.2},\n",
    "    \"Dense-RAG\": {\"Selection\": 45.8, \"Generation\": 150.3},\n",
    "}\n",
    "\n",
    "df_latency = pd.DataFrame(latency_data).T\n",
    "df_latency[\"Total\"] = df_latency[\"Selection\"] + df_latency[\"Generation\"]\n",
    "\n",
    "# Stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df_latency[[\"Selection\", \"Generation\"]].plot(\n",
    "    kind=\"bar\", stacked=True, ax=ax,\n",
    "    color=[\"#3498db\", \"#e74c3c\"]\n",
    ")\n",
    "ax.set_ylabel(\"Latency (ms)\")\n",
    "ax.set_title(\"Latency Comparison\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results: dict, save_path: str):\n",
    "    \"\"\"결과 저장\"\"\"\n",
    "    save_path = Path(save_path)\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {save_path}\")\n",
    "\n",
    "# 저장 예시\n",
    "# all_results = {\n",
    "#     \"qa_metrics\": {\"EM\": 45.2, \"F1\": 52.3},\n",
    "#     \"efficiency\": {\"latency_ms\": 125.7, \"storage_mb\": 39.1},\n",
    "#     \"ragas\": {\"faithfulness\": 0.85, \"answer_relevancy\": 0.78},\n",
    "# }\n",
    "# save_results(all_results, PROJECT_ROOT / \"results\" / \"phase1_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
