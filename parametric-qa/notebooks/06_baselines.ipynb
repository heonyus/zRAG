{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Baseline Comparisons\n",
    "\n",
    "7개 Baseline과의 공정 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\".\").resolve().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline 목록\n",
    "\n",
    "| # | Baseline | Description |\n",
    "|---|----------|-------------|\n",
    "| 1 | No Retrieval | LLM only (parametric knowledge) |\n",
    "| 2 | BM25 + RAG | Sparse retrieval |\n",
    "| 3 | Standard RAG | Dense retrieval (Contriever) |\n",
    "| 4 | Self-RAG | Retrieval-augmented with self-reflection |\n",
    "| 5 | IRCoT | Chain-of-thought with interleaved retrieval |\n",
    "| 6 | Adaptive-RAG | Query complexity based routing |\n",
    "| 7 | CoRAG | Collaborative RAG |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "config_path = PROJECT_ROOT / \"configs\" / \"phase3_full.yaml\"\n",
    "with open(config_path) as f:\n",
    "    config = OmegaConf.create(yaml.safe_load(f))\n",
    "\n",
    "print(\"Baselines config:\")\n",
    "print(OmegaConf.to_yaml(config.baselines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline 1: No Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.no_retrieval import NoRetrievalBaseline\n",
    "\n",
    "class NoRetrievalBaseline:\n",
    "    \"\"\"\n",
    "    Baseline 1: No Retrieval\n",
    "    - LLM의 parametric knowledge만 사용\n",
    "    - 외부 문서 없이 질문에 직접 답변\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_name: str, device: str = \"cuda\"):\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    \n",
    "    def generate(self, question: str, max_new_tokens: int = 64):\n",
    "        prompt = f\"Question: {question}\\nAnswer:\"\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer = generated.split(\"Answer:\")[-1].strip()\n",
    "        return answer\n",
    "\n",
    "print(\"NoRetrievalBaseline defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "# no_retrieval = NoRetrievalBaseline(config.model.llm_name)\n",
    "# answer = no_retrieval.generate(\"What is the capital of France?\")\n",
    "# print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline 2-3: BM25 & Dense RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.standard_rag import StandardRAG\n",
    "\n",
    "class StandardRAG:\n",
    "    \"\"\"\n",
    "    Baseline 2-3: Standard RAG\n",
    "    - BM25 (sparse) or Contriever (dense) retrieval\n",
    "    - Top-k documents → LLM\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_name: str,\n",
    "        corpus: dict,\n",
    "        retriever_type: str = \"dense\",  # \"bm25\" or \"dense\"\n",
    "        top_k: int = 5,\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.corpus = corpus\n",
    "        self.top_k = top_k\n",
    "        self.retriever_type = retriever_type\n",
    "        \n",
    "        # LLM\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Retriever 초기화\n",
    "        self._init_retriever()\n",
    "    \n",
    "    def _init_retriever(self):\n",
    "        if self.retriever_type == \"bm25\":\n",
    "            from rank_bm25 import BM25Okapi\n",
    "            \n",
    "            self.doc_ids = list(self.corpus.keys())\n",
    "            tokenized_corpus = [doc.split() for doc in self.corpus.values()]\n",
    "            self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        else:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            import numpy as np\n",
    "            \n",
    "            self.encoder = SentenceTransformer(\"facebook/contriever\")\n",
    "            self.doc_ids = list(self.corpus.keys())\n",
    "            self.doc_embeddings = self.encoder.encode(\n",
    "                list(self.corpus.values()),\n",
    "                show_progress_bar=True,\n",
    "            )\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = None):\n",
    "        k = k or self.top_k\n",
    "        \n",
    "        if self.retriever_type == \"bm25\":\n",
    "            tokenized_query = query.split()\n",
    "            scores = self.bm25.get_scores(tokenized_query)\n",
    "            top_indices = scores.argsort()[-k:][::-1]\n",
    "        else:\n",
    "            import numpy as np\n",
    "            query_emb = self.encoder.encode([query])\n",
    "            scores = np.dot(self.doc_embeddings, query_emb.T).squeeze()\n",
    "            top_indices = scores.argsort()[-k:][::-1]\n",
    "        \n",
    "        return [self.doc_ids[i] for i in top_indices]\n",
    "    \n",
    "    def generate(self, question: str, max_new_tokens: int = 64):\n",
    "        # Retrieve\n",
    "        retrieved_ids = self.retrieve(question)\n",
    "        context = \"\\n\".join([self.corpus[did] for did in retrieved_ids])\n",
    "        \n",
    "        # Generate\n",
    "        prompt = f\"Context: {context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            prompt, return_tensors=\"pt\", truncation=True, max_length=2048\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "            )\n",
    "        \n",
    "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer = generated.split(\"Answer:\")[-1].strip()\n",
    "        return answer, retrieved_ids\n",
    "\n",
    "print(\"StandardRAG defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline 4-7: Advanced RAG Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-RAG, IRCoT, Adaptive-RAG, CoRAG는\n",
    "# 복잡한 구현이 필요하므로 논문 참조 구현 사용\n",
    "\n",
    "print(\"Advanced RAG baselines:\")\n",
    "print(\"\")\n",
    "print(\"4. Self-RAG (Asai et al., 2023)\")\n",
    "print(\"   - Retrieval-augmented with self-reflection tokens\")\n",
    "print(\"   - [Retrieve], [IsRel], [IsSup], [IsUse] 토큰 사용\")\n",
    "print(\"\")\n",
    "print(\"5. IRCoT (Trivedi et al., 2022)\")\n",
    "print(\"   - Interleaved Retrieval Chain-of-Thought\")\n",
    "print(\"   - 추론 중간에 retrieval 수행\")\n",
    "print(\"\")\n",
    "print(\"6. Adaptive-RAG (Jeong et al., 2024)\")\n",
    "print(\"   - Query complexity 기반 routing\")\n",
    "print(\"   - Simple → No retrieval, Complex → Multi-step\")\n",
    "print(\"\")\n",
    "print(\"7. CoRAG (Collaborative RAG)\")\n",
    "print(\"   - Multiple retriever ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-RAG 간소화 버전\n",
    "class SelfRAGSimplified:\n",
    "    \"\"\"\n",
    "    Self-RAG 간소화 구현\n",
    "    - 실제로는 fine-tuned model이 필요\n",
    "    - 여기서는 retrieval decision을 rule-based로 수행\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_name: str, corpus: dict, device: str = \"cuda\"):\n",
    "        self.rag = StandardRAG(llm_name, corpus, retriever_type=\"dense\", device=device)\n",
    "    \n",
    "    def should_retrieve(self, question: str) -> bool:\n",
    "        \"\"\"Retrieval이 필요한지 판단 (rule-based)\"\"\"\n",
    "        factual_keywords = [\"who\", \"what\", \"when\", \"where\", \"which\", \"how many\"]\n",
    "        q_lower = question.lower()\n",
    "        return any(kw in q_lower for kw in factual_keywords)\n",
    "    \n",
    "    def generate(self, question: str, max_new_tokens: int = 64):\n",
    "        if self.should_retrieve(question):\n",
    "            return self.rag.generate(question, max_new_tokens)\n",
    "        else:\n",
    "            # No retrieval path\n",
    "            prompt = f\"Question: {question}\\nAnswer:\"\n",
    "            inputs = self.rag.tokenizer(prompt, return_tensors=\"pt\").to(self.rag.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.rag.model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "            generated = self.rag.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return generated.split(\"Answer:\")[-1].strip(), []\n",
    "\n",
    "print(\"SelfRAGSimplified defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 통합 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.metrics import compute_em, compute_f1, compute_recall_at_k\n",
    "\n",
    "def evaluate_baseline(baseline, qa_pairs, top_k=5):\n",
    "    \"\"\"\n",
    "    Baseline 평가\n",
    "    \"\"\"\n",
    "    all_em = []\n",
    "    all_f1 = []\n",
    "    all_recall = []\n",
    "    \n",
    "    for item in tqdm(qa_pairs, desc=\"Evaluating\"):\n",
    "        question = item[\"question\"]\n",
    "        answer = item[\"answer\"]\n",
    "        gold_doc_ids = item.get(\"gold_doc_ids\", [])\n",
    "        \n",
    "        # Generate\n",
    "        result = baseline.generate(question)\n",
    "        if isinstance(result, tuple):\n",
    "            prediction, retrieved_ids = result\n",
    "        else:\n",
    "            prediction = result\n",
    "            retrieved_ids = []\n",
    "        \n",
    "        # Metrics\n",
    "        em = compute_em(prediction, answer)\n",
    "        f1 = compute_f1(prediction, answer)\n",
    "        recall = compute_recall_at_k(retrieved_ids, gold_doc_ids, k=top_k) if gold_doc_ids else 0\n",
    "        \n",
    "        all_em.append(em)\n",
    "        all_f1.append(f1)\n",
    "        all_recall.append(recall)\n",
    "    \n",
    "    return {\n",
    "        \"EM\": sum(all_em) / len(all_em) * 100,\n",
    "        \"F1\": sum(all_f1) / len(all_f1) * 100,\n",
    "        f\"Recall@{top_k}\": sum(all_recall) / len(all_recall) * 100 if all_recall else 0,\n",
    "    }\n",
    "\n",
    "print(\"evaluate_baseline defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 baseline 평가 실행\n",
    "# baselines = {\n",
    "#     \"No Retrieval\": no_retrieval_baseline,\n",
    "#     \"BM25-RAG\": bm25_rag,\n",
    "#     \"Dense-RAG\": dense_rag,\n",
    "#     \"Self-RAG\": self_rag,\n",
    "#     \"Parametric-QA\": parametric_qa_model,\n",
    "# }\n",
    "\n",
    "# results = {}\n",
    "# for name, baseline in baselines.items():\n",
    "#     print(f\"\\nEvaluating {name}...\")\n",
    "#     results[name] = evaluate_baseline(baseline, qa_pairs)\n",
    "#     print(f\"Results: {results[name]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 결과\n",
    "example_results = {\n",
    "    \"No Retrieval\": {\"EM\": 22.1, \"F1\": 28.4, \"Recall@5\": 0.0},\n",
    "    \"BM25-RAG\": {\"EM\": 35.6, \"F1\": 42.1, \"Recall@5\": 58.2},\n",
    "    \"Dense-RAG\": {\"EM\": 40.3, \"F1\": 48.7, \"Recall@5\": 72.4},\n",
    "    \"Self-RAG\": {\"EM\": 42.8, \"F1\": 51.2, \"Recall@5\": 74.1},\n",
    "    \"IRCoT\": {\"EM\": 44.1, \"F1\": 53.5, \"Recall@5\": 76.3},\n",
    "    \"Adaptive-RAG\": {\"EM\": 43.5, \"F1\": 52.8, \"Recall@5\": 75.2},\n",
    "    \"CoRAG\": {\"EM\": 45.2, \"F1\": 54.1, \"Recall@5\": 78.5},\n",
    "    \"Parametric-QA (Ours)\": {\"EM\": 47.3, \"F1\": 56.8, \"Recall@5\": 82.1},\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(example_results).T\n",
    "df = df.sort_values(\"EM\", ascending=False)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = range(len(df))\n",
    "width = 0.25\n",
    "\n",
    "colors = [\"#e74c3c\" if \"Ours\" in name else \"#3498db\" for name in df.index]\n",
    "\n",
    "bars1 = ax.bar([i - width for i in x], df[\"EM\"], width, label=\"EM\", color=colors, alpha=0.8)\n",
    "bars2 = ax.bar([i for i in x], df[\"F1\"], width, label=\"F1\", color=colors, alpha=0.6)\n",
    "bars3 = ax.bar([i + width for i in x], df[\"Recall@5\"], width, label=\"Recall@5\", color=colors, alpha=0.4)\n",
    "\n",
    "ax.set_ylabel(\"Score (%)\")\n",
    "ax.set_title(\"Baseline Comparison\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df.index, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Latex Table 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table(df, caption=\"Baseline Comparison\"):\n",
    "    \"\"\"LaTeX 테이블 생성\"\"\"\n",
    "    \n",
    "    latex = \"\\\\begin{table}[h]\\n\"\n",
    "    latex += \"\\\\centering\\n\"\n",
    "    latex += f\"\\\\caption{{{caption}}}\\n\"\n",
    "    latex += \"\\\\begin{tabular}{lccc}\\n\"\n",
    "    latex += \"\\\\toprule\\n\"\n",
    "    latex += \"Method & EM & F1 & Recall@5 \\\\\\\\\\n\"\n",
    "    latex += \"\\\\midrule\\n\"\n",
    "    \n",
    "    for method, row in df.iterrows():\n",
    "        if \"Ours\" in method:\n",
    "            latex += f\"\\\\textbf{{{method}}} & \\\\textbf{{{row['EM']:.1f}}} & \\\\textbf{{{row['F1']:.1f}}} & \\\\textbf{{{row['Recall@5']:.1f}}} \\\\\\\\\\n\"\n",
    "        else:\n",
    "            latex += f\"{method} & {row['EM']:.1f} & {row['F1']:.1f} & {row['Recall@5']:.1f} \\\\\\\\\\n\"\n",
    "    \n",
    "    latex += \"\\\\bottomrule\\n\"\n",
    "    latex += \"\\\\end{tabular}\\n\"\n",
    "    latex += \"\\\\end{table}\"\n",
    "    \n",
    "    return latex\n",
    "\n",
    "print(generate_latex_table(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
