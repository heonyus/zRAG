"""
Z-Only NLL Evaluation
- doc ì…ë ¥ ì—†ì´ zë§Œìœ¼ë¡œ ë¬¸ì„œë¥¼ ì˜ˆì¸¡í•˜ëŠ” NLL ì¸¡ì •
- í˜„ì¬ í•™ìŠµëœ z_pool/projectionì´ ì‹¤ì œë¡œ ë¬¸ì„œ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ”ì§€ í™•ì¸

íŒì • ê¸°ì¤€:
- z-only NLLì´ 8~11: zê°€ ë¬¸ì„œ ì •ë³´ë¥¼ ë‹´ì§€ ì•ŠìŒ (unconditional prior ìˆ˜ì¤€)
- z-only NLLì´ 2~4: zê°€ ë¬¸ì„œ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŒ (conditioning ì‘ë™)
"""

import sys
sys.path.insert(0, "/home/lhe339/data/zRAG")

import torch
import torch.nn.functional as F
from torch.amp import autocast
from pathlib import Path
from omegaconf import OmegaConf
from datasets import load_dataset
from models.write_phase_model import WritePhaseModel, ZPoolManager
from training.train_write_phase import prepare_corpus


def compute_z_only_nll(model, z_i, doc_ids):
    """
    zë§Œìœ¼ë¡œ docì„ ì˜ˆì¸¡í•˜ëŠ” NLL ê³„ì‚°

    Args:
        model: WritePhaseModel
        z_i: [m_tokens, z_dim] learned z vector
        doc_ids: [1, doc_len] document token ids

    Returns:
        nll: negative log likelihood (per token)
    """
    # zë¥¼ embedding spaceë¡œ projection
    alpha_clamped = torch.clamp(model.alpha, min=0.5)
    z_embed = alpha_clamped * model.z_to_embedding(z_i)  # [m_tokens, hidden]
    z_embed = z_embed.unsqueeze(0)  # [1, m_tokens, hidden]

    m_tokens = z_embed.shape[1]
    doc_len = doc_ids.shape[1]

    # z_embedë§Œ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš© (doc_embed ì—†ìŒ!)
    inputs_embeds = z_embed  # [1, m_tokens, hidden]

    # attention mask: z tokensë§Œ
    attention_mask = torch.ones(1, m_tokens, device=z_embed.device)

    # LLM forward
    outputs = model.llm(
        inputs_embeds=inputs_embeds,
        attention_mask=attention_mask,
        use_cache=False,
    )

    # logits: [1, m_tokens, vocab_size]
    # zì˜ ë§ˆì§€ë§‰ í† í° â†’ doc[0] ì˜ˆì¸¡
    # z[-2] â†’ doc[0], z[-1] â†’ doc[1], ...
    # ì‹¤ì œë¡œëŠ” m_tokensê°œì˜ logitsìœ¼ë¡œ m_tokensê°œì˜ doc í† í°ë§Œ ì˜ˆì¸¡ ê°€ëŠ¥
    # ì „ì²´ docì„ ì˜ˆì¸¡í•˜ë ¤ë©´ autoregressiveí•˜ê²Œ í•´ì•¼ í•˜ì§€ë§Œ,
    # ì—¬ê¸°ì„œëŠ” "zê°€ doc ì‹œì‘ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ”ê°€"ë¥¼ ì¸¡ì •

    # ê°„ë‹¨í•œ ë°©ë²•: zì˜ ë§ˆì§€ë§‰ ìœ„ì¹˜ì—ì„œ doc ì²« í† í° ì˜ˆì¸¡
    last_logit = outputs.logits[0, -1, :]  # [vocab_size]
    first_doc_token = doc_ids[0, 0]  # scalar

    nll_first = F.cross_entropy(last_logit.unsqueeze(0), first_doc_token.unsqueeze(0))

    # ë” ì •í™•í•œ ë°©ë²•: autoregressiveë¡œ ì „ì²´ doc NLL ì¸¡ì •
    # doc í† í°ì„ í•˜ë‚˜ì”© ë¶™ì—¬ê°€ë©° loss ê³„ì‚°
    total_nll = 0.0
    num_tokens = min(doc_len, 50)  # ì²˜ìŒ 50í† í°ë§Œ (ì†ë„ ìœ„í•´)

    current_embeds = z_embed.clone()

    for i in range(num_tokens):
        # forward
        outputs = model.llm(
            inputs_embeds=current_embeds,
            use_cache=False,
        )

        # ë§ˆì§€ë§‰ ìœ„ì¹˜ì—ì„œ ë‹¤ìŒ í† í° ì˜ˆì¸¡
        logits = outputs.logits[0, -1, :]  # [vocab_size]
        target = doc_ids[0, i]

        nll = F.cross_entropy(logits.unsqueeze(0), target.unsqueeze(0))
        total_nll += nll.item()

        # ë‹¤ìŒ í† í° embedding ì¶”ê°€
        next_embed = model.llm.get_input_embeddings()(doc_ids[0, i:i+1]).unsqueeze(0)  # [1, 1, hidden]
        current_embeds = torch.cat([current_embeds, next_embed], dim=1)

    avg_nll = total_nll / num_tokens

    return {
        "nll_first_token": nll_first.item(),
        "nll_avg_50": avg_nll,
        "num_tokens": num_tokens,
    }


def main():
    print("=" * 60)
    print("Z-Only NLL Evaluation")
    print("=" * 60)
    print("ëª©ì : zë§Œìœ¼ë¡œ ë¬¸ì„œë¥¼ ì˜ˆì¸¡í•˜ëŠ” NLL ì¸¡ì •")
    print("      (doc ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ìˆœìˆ˜í•˜ê²Œ zì˜ ì •ë³´ëŸ‰ í™•ì¸)")

    # 1. Load config & model
    config_path = "/home/lhe339/data/zRAG/configs/phase1_write.yaml"
    config = OmegaConf.load(config_path)

    print("\n[1] Loading model...")
    model = WritePhaseModel(
        llm_name=config.model.llm_name,
        m_tokens=config.memory.m_tokens,
        z_dim=config.memory.z_dim,
        quantization=config.model.get("quantization", "4bit"),
    )

    # Load trained projection
    proj_path = Path(config.logging.save_dir) / "projection.pt"
    if proj_path.exists():
        model.load_projection(proj_path)
        print(f"  Loaded projection from {proj_path}")

    # Load z_pool
    z_pool_path = Path(config.logging.save_dir) / "z_pool.pt"
    z_pool = ZPoolManager(m_tokens=config.memory.m_tokens, z_dim=config.memory.z_dim)
    z_pool.load(z_pool_path)
    print(f"  Loaded z_pool: {len(z_pool.doc_ids)} documents")
    print(f"  Alpha value: {model.alpha.item():.4f}")

    # Load corpus
    dataset_name = config.data.get("dataset", "hotpot_qa")
    num_docs = config.data.get("num_docs", 10)
    print(f"  Loading dataset: {dataset_name}")
    dataset = load_dataset("hotpotqa/hotpot_qa", "fullwiki")
    corpus = prepare_corpus(dataset, max_docs=num_docs, dataset_name=dataset_name)

    # 2. Prepare test data
    print("\n[2] Preparing test data...")
    doc_ids_list = z_pool.doc_ids[:5]

    tokenizer = model.tokenizer
    tokenized_docs = {}
    for doc_id in doc_ids_list:
        text = corpus[doc_id]
        encoded = tokenizer(
            text,
            return_tensors="pt",
            max_length=config.data.get("max_doc_length", 512),
            truncation=True,
            padding=False,
        )
        tokenized_docs[doc_id] = encoded["input_ids"].cuda()

    print(f"  Prepared {len(doc_ids_list)} documents")

    # 3. Z-Only NLL Evaluation
    print("\n" + "=" * 60)
    print("[3] Z-ONLY NLL EVALUATION")
    print("=" * 60)

    model.eval()

    results_first_token = []  # ì²« í† í° NLL (pure z-only)
    results_z_only = []       # 50 í† í° í‰ê·  (teacher forcing í¬í•¨)
    results_with_doc = []     # doc context ìˆì„ ë•Œ

    with torch.no_grad(), autocast('cuda', dtype=torch.bfloat16):
        for doc_id in doc_ids_list:
            z_i = z_pool.get_z(doc_id).to(model.device)
            doc_ids = tokenized_docs[doc_id]

            # Z-only NLL
            z_only_result = compute_z_only_nll(model, z_i, doc_ids)
            results_first_token.append(z_only_result["nll_first_token"])
            results_z_only.append(z_only_result["nll_avg_50"])

            # With-doc NLL (ê¸°ì¡´ forward ë°©ì‹)
            attention_mask = torch.ones_like(doc_ids)
            out_with_doc = model(z_i, doc_ids, attention_mask)
            loss_with_doc = out_with_doc["loss"].item()
            results_with_doc.append(loss_with_doc)

            print(f"  {doc_id}:")
            print(f"    â˜… nll_first_token (PURE z-only): {z_only_result['nll_first_token']:.4f}")
            print(f"    nll_avg_50 (teacher forcing):    {z_only_result['nll_avg_50']:.4f}")
            print(f"    with-doc NLL:                    {loss_with_doc:.4f}")

    # Summary
    avg_first_token = sum(results_first_token) / len(results_first_token)
    avg_z_only = sum(results_z_only) / len(results_z_only)
    avg_with_doc = sum(results_with_doc) / len(results_with_doc)

    print("\n" + "-" * 40)
    print("[Summary]")
    print(f"  â˜… avg nll_first_token (PURE z-only): {avg_first_token:.4f}")
    print(f"  avg nll_avg_50 (teacher forcing):    {avg_z_only:.4f}")
    print(f"  avg with-doc NLL:                    {avg_with_doc:.4f}")
    print(f"\n  first_token vs with-doc gap: {avg_first_token - avg_with_doc:+.4f}")

    # 4. Diagnosis (based on PURE z-only = first_token)
    print("\n" + "=" * 60)
    print("[4] DIAGNOSIS (based on nll_first_token)")
    print("=" * 60)

    if avg_first_token > 10:
        print("ğŸ”´ z-only NLLì´ ë§¤ìš° ë†’ìŒ (>10)")
        print("   â†’ zê°€ ë¬¸ì„œ ì •ë³´ë¥¼ ë‹´ê³  ìˆì§€ ì•ŠìŒ (unconditional prior ìˆ˜ì¤€)")
        print("   â†’ z-only objectiveë¡œ Phase 1 ì¬í•™ìŠµ í•„ìš”")
    elif avg_first_token > 7:
        print("ğŸŸ¡ z-only NLLì´ ë†’ì€ í¸ (7~10)")
        print("   â†’ zê°€ ì•½ê°„ì˜ ì •ë³´ë¥¼ ë‹´ì§€ë§Œ ë¶€ì¡±í•¨")
        print("   â†’ z-only objective + capacity ì¦ê°€ í•„ìš”")
    elif avg_first_token > 4:
        print("ğŸŸ¢ z-only NLLì´ í•©ë¦¬ì  (4~7)")
        print("   â†’ zê°€ ë¬¸ì„œ ì •ë³´ë¥¼ ì¼ë¶€ ë‹´ê³  ìˆìŒ")
        print("   â†’ fine-tuningìœ¼ë¡œ ê°œì„  ê°€ëŠ¥")
    else:
        print("ğŸŸ¢ z-only NLLì´ ë‚®ìŒ (<4)")
        print("   â†’ zê°€ ë¬¸ì„œ ì •ë³´ë¥¼ ì˜ ë‹´ê³  ìˆìŒ!")

    # 5. Baseline comparison (random z)
    print("\n" + "=" * 60)
    print("[5] BASELINE: Random Z (first_token comparison)")
    print("=" * 60)

    results_random_first = []
    with torch.no_grad(), autocast('cuda', dtype=torch.bfloat16):
        for doc_id in doc_ids_list:
            # Random z (same shape as learned z)
            z_random = torch.randn_like(z_pool.get_z(doc_id)).to(model.device)
            z_random = z_random * 0.1  # scale down
            doc_ids = tokenized_docs[doc_id]

            random_result = compute_z_only_nll(model, z_random, doc_ids)
            results_random_first.append(random_result["nll_first_token"])

    avg_random_first = sum(results_random_first) / len(results_random_first)

    print(f"  â˜… avg random-z first_token NLL:  {avg_random_first:.4f}")
    print(f"  â˜… avg learned-z first_token NLL: {avg_first_token:.4f}")
    print(f"  improvement: {avg_random_first - avg_first_token:+.4f} ({(avg_random_first - avg_first_token) / avg_random_first * 100:+.1f}%)")

    if avg_first_token < avg_random_first - 0.5:
        print("\n  âœ“ í•™ìŠµëœ zê°€ random zë³´ë‹¤ ìœ ì˜ë¯¸í•˜ê²Œ ì¢‹ìŒ")
    else:
        print("\n  âš ï¸ í•™ìŠµëœ zê°€ random zì™€ í° ì°¨ì´ ì—†ìŒ")

    # 6. Z Permutation Test (first_token)
    print("\n" + "=" * 60)
    print("[6] Z PERMUTATION TEST (first_token)")
    print("=" * 60)
    print("ëª©ì : doc_iì— z_jë¥¼ ë„£ì—ˆì„ ë•Œ first_token NLLì´ ì¦ê°€í•˜ëŠ”ì§€ í™•ì¸")

    results_matched_first = []
    results_mismatched_first = []

    with torch.no_grad(), autocast('cuda', dtype=torch.bfloat16):
        for i, doc_id in enumerate(doc_ids_list):
            other_doc_id = doc_ids_list[(i + 1) % len(doc_ids_list)]

            z_matched = z_pool.get_z(doc_id).to(model.device)
            z_mismatched = z_pool.get_z(other_doc_id).to(model.device)
            doc_ids = tokenized_docs[doc_id]

            matched_result = compute_z_only_nll(model, z_matched, doc_ids)
            mismatched_result = compute_z_only_nll(model, z_mismatched, doc_ids)

            results_matched_first.append(matched_result["nll_first_token"])
            results_mismatched_first.append(mismatched_result["nll_first_token"])

            print(f"  {doc_id}: matched={matched_result['nll_first_token']:.4f}, mismatched={mismatched_result['nll_first_token']:.4f}, diff={mismatched_result['nll_first_token'] - matched_result['nll_first_token']:+.4f}")

    avg_matched_first = sum(results_matched_first) / len(results_matched_first)
    avg_mismatched_first = sum(results_mismatched_first) / len(results_mismatched_first)

    print(f"\n  â˜… avg matched first_token NLL:    {avg_matched_first:.4f}")
    print(f"  â˜… avg mismatched first_token NLL: {avg_mismatched_first:.4f}")
    print(f"  gap: {avg_mismatched_first - avg_matched_first:+.4f}")

    if avg_mismatched_first > avg_matched_first + 0.5:
        print("\n  âœ“ zê°€ ë¬¸ì„œë³„ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŒ (mismatchedì—ì„œ NLL ì¦ê°€)")
    else:
        print("\n  âš ï¸ zê°€ ë¬¸ì„œë³„ ì •ë³´ë¥¼ ë‹´ì§€ ì•ŠìŒ (mismatchedì™€ ì°¨ì´ ì—†ìŒ)")


if __name__ == "__main__":
    main()
