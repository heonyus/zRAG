"""
Z Ranking Test
- ê° z_iì— ëŒ€í•´ ëª¨ë“  ë¬¸ì„œ D_jì˜ NLLì„ ê³„ì‚°
- ì •ë‹µ ë¬¸ì„œ D_iê°€ ê°€ì¥ ë‚®ì€ NLLì„ ê°–ëŠ”ì§€ í™•ì¸ (top-1 accuracy)

ê²°ê³¼ í•´ì„:
- top-1 accuracy = 10% (random baseline for 10 docs)
- top-1 accuracy >> 10%: zê°€ ë¬¸ì„œ contentë¥¼ ë‹´ê³  ìˆìŒ
- top-1 accuracy ~ 10%: zê°€ contentë¥¼ ë‹´ì§€ ì•ŠìŒ (ê·¸ëƒ¥ LLM-compatible prefix)
"""

import sys
sys.path.insert(0, "/home/lhe339/data/zRAG")

import torch
import torch.nn.functional as F
from torch.amp import autocast
from pathlib import Path
from omegaconf import OmegaConf
from datasets import load_dataset
from models.write_phase_model import WritePhaseModel, ZPoolManager
from training.train_write_phase import prepare_corpus


def compute_doc_nll(model, z_i, doc_ids, max_tokens=50):
    """
    z_iê°€ ì£¼ì–´ì¡Œì„ ë•Œ docì˜ NLL ê³„ì‚° (teacher forcing)

    Args:
        model: WritePhaseModel
        z_i: [m_tokens, z_dim] z vector
        doc_ids: [1, doc_len] document token ids
        max_tokens: ìµœëŒ€ ëª‡ í† í°ê¹Œì§€ ê³„ì‚°í• ì§€

    Returns:
        nll: average NLL over tokens
    """
    # zë¥¼ embedding spaceë¡œ projection
    alpha_clamped = torch.clamp(model.alpha, min=0.5)
    z_embed = alpha_clamped * model.z_to_embedding(z_i)  # [m_tokens, hidden]
    z_embed = z_embed.unsqueeze(0)  # [1, m_tokens, hidden]

    doc_len = doc_ids.shape[1]
    num_tokens = min(doc_len, max_tokens)

    total_nll = 0.0
    current_embeds = z_embed.clone()

    for i in range(num_tokens):
        # forward
        outputs = model.llm(
            inputs_embeds=current_embeds,
            use_cache=False,
        )

        # ë§ˆì§€ë§‰ ìœ„ì¹˜ì—ì„œ ë‹¤ìŒ í† í° ì˜ˆì¸¡
        logits = outputs.logits[0, -1, :]  # [vocab_size]
        target = doc_ids[0, i]

        nll = F.cross_entropy(logits.unsqueeze(0), target.unsqueeze(0))
        total_nll += nll.item()

        # ë‹¤ìŒ í† í° embedding ì¶”ê°€ (teacher forcing)
        next_embed = model.llm.get_input_embeddings()(doc_ids[0, i:i+1]).unsqueeze(0)
        current_embeds = torch.cat([current_embeds, next_embed], dim=1)

    return total_nll / num_tokens


def main():
    print("=" * 60)
    print("Z Ranking Test")
    print("=" * 60)
    print("ëª©ì : z_iê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì •ë‹µ ë¬¸ì„œ D_ië¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ”ê°€?")
    print("      (top-1 accuracy ì¸¡ì •)")

    # 1. Load config & model
    config_path = "/home/lhe339/data/zRAG/configs/phase1_write.yaml"
    config = OmegaConf.load(config_path)

    print("\n[1] Loading model...")
    model = WritePhaseModel(
        llm_name=config.model.llm_name,
        m_tokens=config.memory.m_tokens,
        z_dim=config.memory.z_dim,
        quantization=config.model.get("quantization", "4bit"),
    )

    # Load trained projection
    proj_path = Path(config.logging.save_dir) / "projection.pt"
    if proj_path.exists():
        model.load_projection(proj_path)
        print(f"  Loaded projection from {proj_path}")

    # Load z_pool
    z_pool_path = Path(config.logging.save_dir) / "z_pool.pt"
    z_pool = ZPoolManager(m_tokens=config.memory.m_tokens, z_dim=config.memory.z_dim)
    z_pool.load(z_pool_path)
    print(f"  Loaded z_pool: {len(z_pool.doc_ids)} documents")
    print(f"  Alpha value: {model.alpha.item():.4f}")

    # Load corpus
    dataset_name = config.data.get("dataset", "hotpot_qa")
    num_docs = config.data.get("num_docs", 10)
    print(f"  Loading dataset: {dataset_name}")
    dataset = load_dataset("hotpotqa/hotpot_qa", "fullwiki")
    corpus = prepare_corpus(dataset, max_docs=num_docs, dataset_name=dataset_name)

    # 2. Prepare test data - ëª¨ë“  ë¬¸ì„œ tokenize
    print("\n[2] Preparing test data...")
    doc_ids_list = z_pool.doc_ids  # ëª¨ë“  ë¬¸ì„œ
    num_test_docs = len(doc_ids_list)

    tokenizer = model.tokenizer
    tokenized_docs = {}
    for doc_id in doc_ids_list:
        text = corpus[doc_id]
        encoded = tokenizer(
            text,
            return_tensors="pt",
            max_length=config.data.get("max_doc_length", 512),
            truncation=True,
            padding=False,
        )
        tokenized_docs[doc_id] = encoded["input_ids"].cuda()

    print(f"  Prepared {num_test_docs} documents")
    print(f"  Random baseline: {1/num_test_docs:.1%} (top-1)")

    # 3. Ranking Test
    print("\n" + "=" * 60)
    print("[3] RANKING TEST")
    print("=" * 60)
    print(f"ê° z_iì— ëŒ€í•´ {num_test_docs}ê°œ ë¬¸ì„œì˜ NLL ê³„ì‚° í›„ ranking")
    print(f"(NLLì´ ë‚®ì„ìˆ˜ë¡ z_iê°€ í•´ë‹¹ ë¬¸ì„œë¥¼ ì˜ ì˜ˆì¸¡í•œë‹¤ëŠ” ì˜ë¯¸)")

    model.eval()

    correct_top1 = 0
    correct_top3 = 0
    all_ranks = []
    all_nll_matrices = []  # ì „ì²´ NLL matrix ì €ì¥

    with torch.no_grad(), autocast('cuda', dtype=torch.bfloat16):
        for i, query_doc_id in enumerate(doc_ids_list):
            z_i = z_pool.get_z(query_doc_id).to(model.device)

            # ëª¨ë“  ë¬¸ì„œì— ëŒ€í•´ NLL ê³„ì‚°
            nlls = {}
            for candidate_doc_id in doc_ids_list:
                doc_ids = tokenized_docs[candidate_doc_id]
                nll = compute_doc_nll(model, z_i, doc_ids, max_tokens=50)
                nlls[candidate_doc_id] = nll

            all_nll_matrices.append(nlls)

            # NLL ê¸°ì¤€ ì •ë ¬ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            sorted_docs = sorted(nlls.items(), key=lambda x: x[1])

            # ì •ë‹µ ë¬¸ì„œì˜ rank ì°¾ê¸°
            rank = -1
            for r, (doc_id, nll) in enumerate(sorted_docs):
                if doc_id == query_doc_id:
                    rank = r + 1  # 1-indexed
                    break

            all_ranks.append(rank)

            if rank == 1:
                correct_top1 += 1
                rank_symbol = "âœ“"
            elif rank <= 3:
                correct_top3 += 1
                rank_symbol = "â–³"
            else:
                rank_symbol = "âœ—"

            # ê²°ê³¼ ì¶œë ¥ (ìƒì„¸)
            correct_nll = nlls[query_doc_id]
            best_nll = sorted_docs[0][1]
            best_doc = sorted_docs[0][0]
            nll_gap = correct_nll - best_nll

            print(f"\n  [{rank_symbol}] z_{i} ({query_doc_id}):")
            print(f"      rank = {rank}/{num_test_docs}")
            print(f"      correct doc NLL = {correct_nll:.4f}")
            print(f"      best doc NLL    = {best_nll:.4f} ({best_doc})")
            print(f"      gap (correct - best) = {nll_gap:+.4f}")

            # top-5 ranking ì¶œë ¥
            top5_str = " > ".join([f"{d}({n:.2f})" for d, n in sorted_docs[:5]])
            print(f"      ranking: {top5_str}")

    # 4. Summary
    print("\n" + "=" * 60)
    print("[4] SUMMARY")
    print("=" * 60)

    top1_acc = correct_top1 / num_test_docs
    top3_acc = correct_top3 / num_test_docs
    avg_rank = sum(all_ranks) / len(all_ranks)
    random_top1 = 1 / num_test_docs
    random_top3 = min(3, num_test_docs) / num_test_docs
    random_avg_rank = (num_test_docs + 1) / 2

    print(f"  Total documents: {num_test_docs}")
    print(f"\n  {'Metric':<25} {'Actual':>10} {'Random':>10} {'Ratio':>10}")
    print(f"  {'-'*55}")
    print(f"  {'Top-1 Accuracy':<25} {top1_acc:>9.1%} {random_top1:>9.1%} {top1_acc/random_top1:>9.1f}x")
    print(f"  {'Top-3 Accuracy':<25} {top3_acc:>9.1%} {random_top3:>9.1%} {top3_acc/random_top3:>9.1f}x")
    print(f"  {'Average Rank':<25} {avg_rank:>10.2f} {random_avg_rank:>10.2f} {random_avg_rank/avg_rank:>9.1f}x")

    print(f"\n  â˜… Top-1 Accuracy: {top1_acc:.1%} ({correct_top1}/{num_test_docs})")
    print(f"  â˜… Top-3 Accuracy: {top3_acc:.1%} ({correct_top3}/{num_test_docs})")
    print(f"  â˜… Average Rank:   {avg_rank:.2f} (random would be {random_avg_rank:.1f})")
    print(f"\n  Rank distribution: {all_ranks}")

    # NLL í†µê³„
    print("\n  --- NLL Statistics ---")
    all_correct_nlls = []
    all_incorrect_nlls = []
    for i, query_doc_id in enumerate(doc_ids_list):
        nlls = all_nll_matrices[i]
        for doc_id, nll in nlls.items():
            if doc_id == query_doc_id:
                all_correct_nlls.append(nll)
            else:
                all_incorrect_nlls.append(nll)

    avg_correct_nll = sum(all_correct_nlls) / len(all_correct_nlls)
    avg_incorrect_nll = sum(all_incorrect_nlls) / len(all_incorrect_nlls)
    nll_separation = avg_incorrect_nll - avg_correct_nll

    print(f"  avg NLL (correct doc):   {avg_correct_nll:.4f}")
    print(f"  avg NLL (incorrect doc): {avg_incorrect_nll:.4f}")
    print(f"  separation gap:          {nll_separation:+.4f}")

    if nll_separation > 0.5:
        print(f"  â†’ ì •ë‹µ ë¬¸ì„œì˜ NLLì´ ìœ ì˜ë¯¸í•˜ê²Œ ë‚®ìŒ (good separation)")
    else:
        print(f"  â†’ ì •ë‹µ/ì˜¤ë‹µ ë¬¸ì„œì˜ NLL ì°¨ì´ê°€ ì‘ìŒ (poor separation)")

    # 5. Diagnosis
    print("\n" + "=" * 60)
    print("[5] DIAGNOSIS")
    print("=" * 60)

    random_baseline = 1 / num_test_docs
    improvement_ratio = top1_acc / random_baseline if random_baseline > 0 else 0

    print(f"\n  [í•µì‹¬ ì§€í‘œ]")
    print(f"  - Top-1 Accuracy: {top1_acc:.1%} (random: {random_baseline:.1%})")
    print(f"  - Improvement:    {improvement_ratio:.1f}x over random")
    print(f"  - NLL Separation: {nll_separation:+.4f}")

    print(f"\n  [íŒì •]")
    if top1_acc >= 0.8:  # 80%+
        print("  ğŸŸ¢ğŸŸ¢ zê°€ ë¬¸ì„œ contentë¥¼ ë§¤ìš° ì˜ ë‹´ê³  ìˆìŒ!")
        print(f"      top-1 accuracy {top1_acc:.1%} (ê±°ì˜ ì™„ë²½)")
        print("      â†’ Phase 2ë¡œ ì§„í–‰ ê°€ëŠ¥")
    elif top1_acc > random_baseline * 5:  # 5x better than random
        print("  ğŸŸ¢ zê°€ ë¬¸ì„œ contentë¥¼ ë‹´ê³  ìˆìŒ!")
        print(f"      top-1 accuracy {top1_acc:.1%} >> random {random_baseline:.1%} ({improvement_ratio:.1f}x)")
        print("      â†’ z-only objectiveë¡œ ë” ê°œì„  ê°€ëŠ¥")
    elif top1_acc > random_baseline * 2:  # 2x better than random
        print("  ğŸŸ¡ zê°€ ì•½ê°„ì˜ content ì •ë³´ë¥¼ ë‹´ìŒ")
        print(f"      top-1 accuracy {top1_acc:.1%} > random {random_baseline:.1%} ({improvement_ratio:.1f}x)")
        print("      â†’ z-only objective í•„ìš”")
    elif top1_acc > random_baseline:  # slightly better
        print("  ğŸŸ  zê°€ contentë¥¼ ê±°ì˜ ë‹´ì§€ ì•ŠìŒ")
        print(f"      top-1 accuracy {top1_acc:.1%} ~ random {random_baseline:.1%} ({improvement_ratio:.1f}x)")
        print("      â†’ í˜„ì¬ objectiveë¡œëŠ” content encoding ì–´ë ¤ì›€")
        print("      â†’ z-only objectiveë¡œ Phase 1 ì¬ì„¤ê³„ í•„ìš”")
    else:
        print("  ğŸ”´ zê°€ contentë¥¼ ì „í˜€ ë‹´ì§€ ì•ŠìŒ")
        print(f"      top-1 accuracy {top1_acc:.1%} â‰¤ random {random_baseline:.1%}")
        print("      â†’ í˜„ì¬ objectiveë¡œëŠ” content encoding ë¶ˆê°€")
        print("      â†’ z-only objectiveë¡œ Phase 1 ì¬ì„¤ê³„ í•„ìˆ˜")

    print(f"\n  [ë‹¤ìŒ ë‹¨ê³„]")
    if top1_acc >= 0.5:
        print("  1. Phase 1ì€ ì–´ëŠ ì •ë„ ì„±ê³µ - capacity/epoch ì¡°ì ˆë¡œ ì¶”ê°€ ê°œì„ ")
        print("  2. ë˜ëŠ” z-only objectiveë¡œ ì „í™˜í•˜ì—¬ ë” ê°•í•œ encoding ì‹œë„")
    else:
        print("  1. z-only objectiveë¡œ Phase 1 ì¬í•™ìŠµ (ìµœìš°ì„ )")
        print("  2. ì§§ì€ ë¬¸ì„œ(64-128 tokens)ë¡œ ì‹œì‘")
        print("  3. train-test mismatch í•´ì†Œ í•„ìˆ˜")


if __name__ == "__main__":
    main()
