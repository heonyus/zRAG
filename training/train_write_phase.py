"""
Phase 1: Write Phase Training Script

êµìˆ˜ë‹˜ ì˜ë„ (2=A):
- z_ië§Œ ë„£ìœ¼ë©´ í•´ë‹¹ ë¬¸ì„œ D_iê°€ ìƒì„±ë˜ë„ë¡ í•™ìŠµ
- LLM freeze, z_i + projectionë§Œ í•™ìŠµ
- ë¬¸ì„œë³„ë¡œ z_ië¥¼ ìµœì í™”í•˜ê³ , ì „ì²´ z_poolë¡œ ì €ì¥

ì‚¬ìš©ë²•:
    python training/train_write_phase.py --config configs/phase1_write.yaml
    python training/train_write_phase.py --config configs/phase1_write.yaml --test  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
"""

import sys
import logging
import argparse
from pathlib import Path
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.amp import autocast, GradScaler
import yaml
from omegaconf import OmegaConf

# Path setup
sys.path.append(str(Path(__file__).parent.parent))

from models.write_phase_model import WritePhaseModel, ZPoolManager
from data.download import download_dataset
from data.dataloader import WritePhaseDataset

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)


def prepare_corpus(dataset, max_docs: int = 200, dataset_name: str = "hotpot_qa") -> dict:
    """
    ë°ì´í„°ì…‹ì—ì„œ corpus ì¶”ì¶œ

    Args:
        dataset: HuggingFace dataset
        max_docs: ìµœëŒ€ ë¬¸ì„œ ìˆ˜
        dataset_name: ë°ì´í„°ì…‹ ì´ë¦„ (hotpot_qa, natural_questions ë“±)

    Returns:
        corpus: {doc_id: doc_text} dict
    """
    corpus = {}

    # train split ì²˜ë¦¬
    if hasattr(dataset, "keys") and "train" in dataset.keys():
        data = dataset["train"]
    else:
        data = dataset

    for i, item in enumerate(data):
        if len(corpus) >= max_docs:
            break

        # HotpotQA format: context = {'title': [...], 'sentences': [[...], ...]}
        if dataset_name == "hotpot_qa" and "context" in item:
            ctx = item["context"]
            titles = ctx.get("title", [])
            sentences_list = ctx.get("sentences", [])

            # ê° ë¬¸ì„œ(title + sentences)ë¥¼ ë³„ë„ ë¬¸ì„œë¡œ ì¶”ì¶œ
            for title, sentences in zip(titles, sentences_list):
                if len(corpus) >= max_docs:
                    break
                doc_text = f"{title}\n" + " ".join(sentences)
                if len(doc_text) > 50:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì„œ ì œì™¸
                    doc_id = f"doc_{len(corpus)}"
                    corpus[doc_id] = doc_text

        # FlashRAG NQ format: retrieval_resultê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì´ ë¬¸ì„œ
        elif "retrieval_result" in item and item["retrieval_result"]:
            for j, doc in enumerate(item["retrieval_result"][:1]):  # ì²« ë²ˆì§¸ ë¬¸ì„œë§Œ
                doc_id = f"doc_{len(corpus)}"
                doc_text = doc.get("contents", doc.get("text", ""))
                if doc_text and len(doc_text) > 50:
                    corpus[doc_id] = doc_text

        # ì¼ë°˜ context (ë¬¸ìì—´)
        elif "context" in item and isinstance(item["context"], str):
            doc_id = f"doc_{len(corpus)}"
            doc_text = item["context"]
            if len(doc_text) > 50:
                corpus[doc_id] = doc_text

    logger.info(f"Extracted {len(corpus)} documents from dataset")
    return corpus


def train_single_document(
    model: WritePhaseModel,
    doc_id: str,
    doc_ids: torch.Tensor,
    doc_attention_mask: torch.Tensor,
    config: dict,
    scaler: GradScaler = None,
    enable_diagnostics: bool = True,
) -> tuple:
    """
    ë‹¨ì¼ ë¬¸ì„œì— ëŒ€í•´ z_ië¥¼ í•™ìŠµ

    Args:
        model: WritePhaseModel (LLM frozen)
        doc_id: ë¬¸ì„œ ID
        doc_ids: [1, doc_len] í† í°í™”ëœ ë¬¸ì„œ
        doc_attention_mask: [1, doc_len]
        config: í•™ìŠµ ì„¤ì •
        scaler: GradScaler for mixed precision
        enable_diagnostics: ì¤‘ê°„ ìƒ˜í”Œ ìƒì„± ë° í†µê³„ ì¶œë ¥ ì—¬ë¶€

    Returns:
        z_i: í•™ìŠµëœ z_i tensor
        final_loss: ìµœì¢… loss
    """
    # ìƒˆ z_i ìƒì„±
    z_i = model.create_z_for_doc()
    z_i_init = z_i.clone().detach()  # ì´ˆê¸°ê°’ ì €ì¥ (ë³€í™”ëŸ‰ ì¸¡ì •ìš©)

    # Learning rates from config
    lr_z = float(config.get("lr_z", 1e-2))
    lr_proj = float(config.get("lr_proj", 0))

    # Optimizer (z_i + projection if lr_proj > 0)
    optimizer = AdamW(
        model.get_trainable_params(z_i, lr_z=lr_z, lr_proj=lr_proj),
        weight_decay=config.get("weight_decay", 0.01),
    )

    # í•™ìŠµ ì„¤ì •
    epochs = config.get("epochs_per_doc", 100)
    log_every = config.get("log_every", 20)
    use_amp = config.get("use_amp", True)
    early_stop_loss = config.get("early_stop_loss", 0.5)

    best_loss = float("inf")
    best_z = z_i.clone().detach()

    # ì§„ë‹¨ìš©: ì¤‘ê°„ ìƒ˜í”Œ ìƒì„±í•  epochë“¤
    diagnostic_epochs = {0, 1, 5, 10, 20, 50, epochs - 1} if enable_diagnostics else set()

    # ì²« ë¬¸ì„œì˜ ì²« epochì—ì„œ ì´ˆê¸° ìƒíƒœ ë¡œê¹…
    if doc_id == "doc_0" and enable_diagnostics:
        stats = model.get_z_embed_stats(z_i)
        logger.info(f"  [{doc_id}] INIT: z_norm={stats['z_i_norm']:.4f}, "
                   f"z_embed_norm={stats['z_embed_norm']:.4f}, z_embed_std={stats['z_embed_std']:.4f}")

    for epoch in range(epochs):
        optimizer.zero_grad()

        if use_amp and scaler is not None:
            with autocast('cuda', dtype=torch.bfloat16):
                outputs = model(z_i, doc_ids, doc_attention_mask)
                loss = outputs["loss"]

            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)

            # Gradient norm ê³„ì‚° (z_ië§Œ)
            z_grad_norm = z_i.grad.norm().item() if z_i.grad is not None else 0.0

            nn.utils.clip_grad_norm_([z_i], 1.0)
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs = model(z_i, doc_ids, doc_attention_mask)
            loss = outputs["loss"]

            loss.backward()
            z_grad_norm = z_i.grad.norm().item() if z_i.grad is not None else 0.0
            nn.utils.clip_grad_norm_([z_i], 1.0)
            optimizer.step()

        loss_val = loss.item()

        # Best ì €ì¥
        if loss_val < best_loss:
            best_loss = loss_val
            best_z = z_i.clone().detach()

        # Early stopping
        if loss_val < early_stop_loss:
            logger.info(f"  [{doc_id}] Early stop at epoch {epoch}, loss={loss_val:.4f}")
            break

        # Logging (every log_every epochs)
        if epoch % log_every == 0 or epoch == epochs - 1:
            z_change = (z_i - z_i_init).norm().item()
            logger.debug(f"  [{doc_id}] Epoch {epoch}/{epochs}: loss={loss_val:.4f}, "
                        f"z_grad={z_grad_norm:.4f}, z_change={z_change:.4f}")

        # ì§„ë‹¨: ì¤‘ê°„ ìƒ˜í”Œ ìƒì„± ë° í†µê³„
        if epoch in diagnostic_epochs and enable_diagnostics:
            # z_embed í†µê³„ ì¶œë ¥
            stats = model.get_z_embed_stats(z_i)
            z_change = (z_i - z_i_init).norm().item()
            logger.info(f"  [{doc_id}] Epoch {epoch}: loss={loss_val:.4f} | "
                       f"z_norm={stats['z_i_norm']:.4f}, z_change={z_change:.4f}, "
                       f"z_grad={z_grad_norm:.4f} | "
                       f"z_embed_norm={stats['z_embed_norm']:.4f}, z_embed_std={stats['z_embed_std']:.4f}")

            # ì²« ë¬¸ì„œë§Œ ì¤‘ê°„ ìƒì„± í…ŒìŠ¤íŠ¸ (ì‹œê°„ ì ˆì•½)
            if doc_id == "doc_0":
                try:
                    sample = model.generate_from_z(z_i.detach(), max_new_tokens=50, do_sample=True)
                    logger.info(f"  [{doc_id}] Epoch {epoch} sample: {sample[:100]}...")
                except Exception as e:
                    logger.warning(f"  [{doc_id}] Epoch {epoch} generate failed: {e}")

    # ìµœì¢… ìƒíƒœ ë¡œê¹…
    final_z_change = (best_z - z_i_init).norm().item()
    logger.debug(f"  [{doc_id}] FINAL: best_loss={best_loss:.4f}, total_z_change={final_z_change:.4f}")

    return best_z, best_loss


def train_shuffled_documents(
    model: WritePhaseModel,
    tokenized_docs: dict,
    z_vectors: dict,
    config: dict,
    scaler: GradScaler = None,
) -> dict:
    """
    Shuffled doc training: projection drift ë°©ì§€ë¥¼ ìœ„í•´ ë¬¸ì„œë“¤ì„ ì„ì–´ì„œ í•™ìŠµ
    """
    import random
    import time
    import statistics

    # === Config ë¡œë“œ ===
    lr_z = float(config.get("lr_z", 1e-2))
    lr_proj = float(config.get("lr_proj", 1e-5))
    epochs = config.get("epochs_per_doc", 100)
    log_every = config.get("log_every", 20)
    use_amp = config.get("use_amp", True)
    early_stop_loss = config.get("early_stop_loss", 0.5)
    collapse_threshold = config.get("collapse_threshold", 0.01)
    stagnation_patience = config.get("stagnation_patience", 5)
    checkpoint_every = config.get("checkpoint_every", 10)

    doc_ids = list(tokenized_docs.keys())
    num_docs = len(doc_ids)
    total_iters = num_docs * epochs

    # === í•™ìŠµ ì„¤ì • ì¶œë ¥ ===
    print("\n" + "=" * 70)
    print("ğŸ“‹ TRAINING CONFIGURATION")
    print("=" * 70)
    print(f"  Documents:     {num_docs}")
    print(f"  Epochs:        {epochs}")
    print(f"  Total iters:   {total_iters:,}")
    print(f"  lr_z:          {lr_z}")
    print(f"  lr_proj:       {lr_proj}")
    print(f"  use_amp:       {use_amp}")
    print(f"  log_every:     {log_every} epochs")
    print(f"  checkpoint:    every {checkpoint_every} epochs")
    print("=" * 70 + "\n")

    # === Optimizer ì„¤ì • ===
    z_params = [z_vectors[doc_id] for doc_id in doc_ids]
    param_groups = [
        {"params": z_params, "lr": lr_z, "weight_decay": config.get("weight_decay", 0.01), "name": "z_vectors"},
        {"params": [model.alpha], "lr": lr_z, "weight_decay": 0.0, "name": "alpha"},
    ]

    if lr_proj > 0:
        param_groups.append({
            "params": model.z_to_embedding.parameters(),
            "lr": lr_proj,
            "weight_decay": 0.0,
            "name": "z_to_embedding"
        })
        print(f"ğŸ”§ Optimizer: z_lr={lr_z}, alpha_lr={lr_z}, proj_lr={lr_proj}")
    else:
        for param in model.z_to_embedding.parameters():
            param.requires_grad = False
        print(f"ğŸ”§ Optimizer: z_lr={lr_z}, alpha_lr={lr_z}, proj=FROZEN")

    optimizer = AdamW(param_groups, weight_decay=0.0)

    # === ìƒíƒœ ì¶”ì  ë³€ìˆ˜ ===
    best_losses = {doc_id: float("inf") for doc_id in doc_ids}
    current_losses = {doc_id: float("inf") for doc_id in doc_ids}
    z_init = {doc_id: z_vectors[doc_id].clone().detach() for doc_id in doc_ids}

    loss_history = []
    best_avg_loss = float("inf")
    stagnation_counter = 0
    collapse_warned = False

    # === ì´ˆê¸° z í†µê³„ ===
    init_z_norms = [z_vectors[d].norm().item() for d in doc_ids]
    init_z_stds = [z_vectors[d].std().item() for d in doc_ids]
    print(f"\nğŸ“Š Initial z stats:")
    print(f"   z_norm: mean={statistics.mean(init_z_norms):.4f}, std={statistics.stdev(init_z_norms) if len(init_z_norms) > 1 else 0:.4f}")
    print(f"   z_std:  mean={statistics.mean(init_z_stds):.4f}")
    print(f"   alpha:  {model.alpha.item():.4f}")

    # === íƒ€ì´ë° ===
    start_time = time.time()
    epoch_times = []

    # === ë©”ì¸ í•™ìŠµ ë£¨í”„ ===
    print("\n" + "=" * 70)
    print("ğŸš€ TRAINING START")
    print("=" * 70)

    # ì „ì²´ ì§„í–‰ë¥  ë°”
    total_pbar = tqdm(
        total=total_iters,
        desc="Total",
        position=0,
        leave=True,
        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
    )

    global_iter = 0

    for epoch in range(epochs):
        epoch_start = time.time()
        random.shuffle(doc_ids)

        epoch_losses = []
        epoch_grad_norms = []

        # Epoch ì§„í–‰ë¥  ë°”
        epoch_pbar = tqdm(
            doc_ids,
            desc=f"Ep {epoch:03d}",
            position=1,
            leave=False,
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}] {postfix}'
        )

        for doc_idx, doc_id in enumerate(epoch_pbar):
            optimizer.zero_grad()

            doc_data = tokenized_docs[doc_id]
            z_i = z_vectors[doc_id]

            # Forward + Backward
            if use_amp and scaler is not None:
                with autocast('cuda', dtype=torch.bfloat16):
                    outputs = model(z_i, doc_data["input_ids"], doc_data["attention_mask"])
                    loss = outputs["loss"]

                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                grad_norm = nn.utils.clip_grad_norm_(z_params + list(model.z_to_embedding.parameters()), 1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                outputs = model(z_i, doc_data["input_ids"], doc_data["attention_mask"])
                loss = outputs["loss"]

                loss.backward()
                grad_norm = nn.utils.clip_grad_norm_(z_params + list(model.z_to_embedding.parameters()), 1.0)
                optimizer.step()

            # í†µê³„ ìˆ˜ì§‘
            loss_val = loss.item()
            grad_norm_val = grad_norm.item() if hasattr(grad_norm, 'item') else grad_norm
            epoch_losses.append(loss_val)
            epoch_grad_norms.append(grad_norm_val)
            current_losses[doc_id] = loss_val

            if loss_val < best_losses[doc_id]:
                best_losses[doc_id] = loss_val

            # Epoch ì§„í–‰ë¥  ë°” ì—…ë°ì´íŠ¸
            running_avg = sum(epoch_losses) / len(epoch_losses)
            epoch_pbar.set_postfix({
                'loss': f'{loss_val:.3f}',
                'avg': f'{running_avg:.3f}',
                'Î±': f'{model.alpha.item():.2f}'
            })

            # ì „ì²´ ì§„í–‰ë¥  ë°” ì—…ë°ì´íŠ¸
            global_iter += 1
            total_pbar.update(1)
            total_pbar.set_postfix({
                'ep': f'{epoch}/{epochs}',
                'loss': f'{running_avg:.3f}',
                'Î±': f'{model.alpha.item():.2f}'
            })

        epoch_pbar.close()

        # === Epoch í†µê³„ ê³„ì‚° ===
        epoch_time = time.time() - epoch_start
        epoch_times.append(epoch_time)

        avg_loss = statistics.mean(epoch_losses)
        loss_std = statistics.stdev(epoch_losses) if len(epoch_losses) > 1 else 0
        loss_min = min(epoch_losses)
        loss_max = max(epoch_losses)

        avg_grad = statistics.mean(epoch_grad_norms)

        # z í†µê³„
        z_norms = [z_vectors[d].norm().item() for d in doc_ids]
        z_stds = [z_vectors[d].std().item() for d in doc_ids]
        z_changes = [(z_vectors[d] - z_init[d]).norm().item() for d in doc_ids]

        avg_z_norm = statistics.mean(z_norms)
        avg_z_std = statistics.mean(z_stds)
        avg_z_change = statistics.mean(z_changes)

        loss_history.append(avg_loss)

        # === Epoch ë¡œê·¸ ì¶œë ¥ ===
        if epoch % log_every == 0 or epoch == epochs - 1 or epoch < 3:
            elapsed = time.time() - start_time
            if epoch > 0:
                eta = (elapsed / (epoch + 1)) * (epochs - epoch - 1)
                eta_str = f"{int(eta // 60):02d}:{int(eta % 60):02d}"
            else:
                eta_str = "--:--"
            elapsed_str = f"{int(elapsed // 60):02d}:{int(elapsed % 60):02d}"

            print(f"\n{'â”€' * 70}")
            print(f"ğŸ“ˆ EPOCH {epoch:03d}/{epochs} | elapsed={elapsed_str} | ETA={eta_str} | {epoch_time:.1f}s/ep")
            print(f"{'â”€' * 70}")
            print(f"  Loss:  avg={avg_loss:.4f} | std={loss_std:.4f} | min={loss_min:.4f} | max={loss_max:.4f}")
            print(f"  Grad:  avg_norm={avg_grad:.4f}")
            print(f"  Alpha: {model.alpha.item():.4f}")
            print(f"  z:     norm={avg_z_norm:.4f} | std={avg_z_std:.4f} | Î”={avg_z_change:.4f}")

            # ê°œì„  ìƒíƒœ
            if epoch > 0:
                improvement = loss_history[-2] - avg_loss
                arrow = "â†“" if improvement > 0 else "â†‘" if improvement < 0 else "â†’"
                print(f"  Î”loss: {arrow} {abs(improvement):.4f} (prev={loss_history[-2]:.4f})")

        # === ì•ˆì „ì¥ì¹˜ ì²´í¬ ===
        # 1. Collapse ê°ì§€
        if avg_z_std < collapse_threshold and not collapse_warned:
            print(f"\nâš ï¸  [COLLAPSE WARNING] z_std={avg_z_std:.6f} < {collapse_threshold}")
            print(f"    z vectorsê°€ ë„ˆë¬´ ë¹„ìŠ·í•´ì§€ê³  ìˆìŒ!")
            collapse_warned = True

        # 2. Stagnation ê°ì§€
        if avg_loss < best_avg_loss - 0.001:
            best_avg_loss = avg_loss
            stagnation_counter = 0
        else:
            stagnation_counter += 1
            if stagnation_counter >= stagnation_patience and stagnation_counter % stagnation_patience == 0:
                print(f"\nâš ï¸  [STAGNATION] {stagnation_counter} epochs without improvement")
                print(f"    best={best_avg_loss:.4f}, current={avg_loss:.4f}")

        # 3. ìƒ˜í”Œ ìƒì„± (íŠ¹ì • epochì—ì„œ)
        if epoch in {0, 1, 5, 10, epochs // 2, epochs - 1}:
            test_doc_id = doc_ids[0]
            try:
                with torch.no_grad():
                    sample = model.generate_from_z(
                        z_vectors[test_doc_id].detach(),
                        max_new_tokens=40,
                        do_sample=True
                    )
                print(f"  Sample: \"{sample[:80]}...\"")
            except Exception as e:
                print(f"  Sample: [failed: {e}]")

        # 4. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if checkpoint_every > 0 and (epoch + 1) % checkpoint_every == 0:
            checkpoint_dir = Path(config.get("save_dir", "./checkpoints/phase1_write"))
            checkpoint_dir.mkdir(parents=True, exist_ok=True)

            checkpoint_path = checkpoint_dir / f"z_pool_epoch{epoch+1}.pt"
            checkpoint_data = {
                "epoch": epoch + 1,
                "z_vectors": {doc_id: z_vectors[doc_id].detach().cpu() for doc_id in doc_ids},
                "avg_loss": avg_loss,
                "alpha": model.alpha.item(),
                "loss_history": loss_history,
            }
            torch.save(checkpoint_data, checkpoint_path)
            print(f"  ğŸ’¾ Checkpoint: {checkpoint_path}")

        # 5. Early stopping
        if avg_loss < early_stop_loss:
            print(f"\nâœ… Early stopping at epoch {epoch}, loss={avg_loss:.4f} < {early_stop_loss}")
            break

    total_pbar.close()

    # === ìµœì¢… ìš”ì•½ ===
    total_time = time.time() - start_time
    final_avg_loss = statistics.mean(list(best_losses.values()))

    print("\n" + "=" * 70)
    print("ğŸ TRAINING COMPLETED")
    print("=" * 70)
    print(f"  Total time:    {int(total_time // 60)}m {int(total_time % 60)}s")
    print(f"  Epochs:        {len(loss_history)}/{epochs}")
    print(f"  Final loss:    {loss_history[-1]:.4f}")
    print(f"  Best avg loss: {final_avg_loss:.4f}")
    print(f"  Final alpha:   {model.alpha.item():.4f}")

    # z ìµœì¢… í†µê³„
    final_z_norms = [z_vectors[d].norm().item() for d in doc_ids]
    final_z_stds = [z_vectors[d].std().item() for d in doc_ids]
    final_z_changes = [(z_vectors[d] - z_init[d]).norm().item() for d in doc_ids]

    print(f"\n  z final stats:")
    print(f"    norm:   {statistics.mean(final_z_norms):.4f} (init: {statistics.mean(init_z_norms):.4f})")
    print(f"    std:    {statistics.mean(final_z_stds):.4f} (init: {statistics.mean(init_z_stds):.4f})")
    print(f"    change: {statistics.mean(final_z_changes):.4f}")

    # Loss ë³€í™”
    if len(loss_history) > 1:
        print(f"\n  Loss trajectory: {loss_history[0]:.3f} â†’ {loss_history[-1]:.3f}")
        print(f"    Reduction: {loss_history[0] - loss_history[-1]:.3f} ({(1 - loss_history[-1]/loss_history[0])*100:.1f}%)")

    print("=" * 70 + "\n")

    return best_losses


def run_write_phase_training(config_path: str = None, config: dict = None, test_mode: bool = False):
    """
    Phase 1: Write Phase ì „ì²´ í•™ìŠµ ì‹¤í–‰

    Args:
        config_path: YAML config íŒŒì¼ ê²½ë¡œ
        config: config dict (ì§ì ‘ ì „ë‹¬ ì‹œ)
        test_mode: Trueë©´ ì†Œê·œëª¨ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸

    Returns:
        model: WritePhaseModel
        z_pool_manager: í•™ìŠµëœ z_ië“¤
        results: í•™ìŠµ ê²°ê³¼
    """
    # Load config
    if config is None:
        with open(config_path, "r") as f:
            config = OmegaConf.create(yaml.safe_load(f))

    # Test mode ì˜¤ë²„ë¼ì´ë“œ
    if test_mode:
        config.data.num_docs = 10
        config.training.epochs_per_doc = 20
        logger.info("=" * 60)
        logger.info("TEST MODE: num_docs=10, epochs_per_doc=20")
        logger.info("=" * 60)

    logger.info("=" * 60)
    logger.info("Phase 1: Write Phase Training (Token-as-Document)")
    logger.info("=" * 60)
    logger.info(f"Config:\n{OmegaConf.to_yaml(config)}")

    # ==========================================
    # 1. Data Preparation
    # ==========================================
    logger.info("\n[Step 1] Data Preparation")

    data_config = config.data
    raw_data = download_dataset(
        dataset_name=data_config.dataset,
        save_dir=data_config.get("save_dir", "./data/raw"),
    )

    # Corpus ì¶”ì¶œ
    corpus = prepare_corpus(
        raw_data,
        max_docs=data_config.num_docs,
        dataset_name=data_config.dataset,
    )
    logger.info(f"Corpus size: {len(corpus)} documents")

    if len(corpus) == 0:
        raise ValueError("No documents extracted from dataset!")

    # ==========================================
    # 2. Model Initialization
    # ==========================================
    logger.info("\n[Step 2] Model Initialization")

    model_config = config.model
    memory_config = config.memory

    model = WritePhaseModel(
        llm_name=model_config.llm_name,
        m_tokens=memory_config.m_tokens,
        z_dim=memory_config.z_dim,
        quantization=model_config.get("quantization", "4bit"),
    )

    # Z Pool Manager
    z_pool_manager = ZPoolManager(
        m_tokens=memory_config.m_tokens,
        z_dim=memory_config.z_dim,
    )

    # ==========================================
    # 3. Tokenize Documents
    # ==========================================
    logger.info("\n[Step 3] Tokenizing Documents")

    tokenizer = model.tokenizer
    max_doc_length = data_config.get("max_doc_length", 512)

    tokenized_docs = {}
    for doc_id, doc_text in tqdm(corpus.items(), desc="Tokenizing"):
        encoded = tokenizer(
            doc_text,
            max_length=max_doc_length,
            truncation=True,
            padding="max_length",
            return_tensors="pt",
        )
        tokenized_docs[doc_id] = {
            "input_ids": encoded["input_ids"].cuda(),
            "attention_mask": encoded["attention_mask"].cuda(),
        }

    logger.info(f"Tokenized {len(tokenized_docs)} documents")

    # ==========================================
    # 4. Training: Shuffled Document Training
    # ==========================================
    logger.info("\n[Step 4] Training z_i with shuffled documents (drift ë°©ì§€)")

    train_config = config.training
    use_amp = train_config.get("use_amp", True)
    scaler = GradScaler('cuda') if use_amp else None

    # Training config ë¡œê¹…
    lr_z = float(train_config.get("lr_z", 1e-2))
    lr_proj = float(train_config.get("lr_proj", 1e-5))
    epochs_per_doc = train_config.get("epochs_per_doc", 100)
    logger.info(f"Training config: lr_z={lr_z}, lr_proj={lr_proj}, epochs={epochs_per_doc}")
    logger.info(f"Projection: {'FROZEN' if lr_proj == 0 else f'learning (lr={lr_proj})'}")
    logger.info(f"Training mode: SHUFFLED (all docs trained together)")

    save_dir = Path(config.logging.get("save_dir", "./checkpoints/phase1_write"))
    save_dir.mkdir(parents=True, exist_ok=True)

    doc_ids_list = list(tokenized_docs.keys())

    # ëª¨ë“  ë¬¸ì„œì— ëŒ€í•´ z_i ìƒì„±
    z_vectors = {}
    for doc_id in tqdm(doc_ids_list, desc="Creating z_i vectors"):
        z_vectors[doc_id] = model.create_z_for_doc()
    logger.info(f"Created {len(z_vectors)} z_i vectors")

    # train_configì— save_dir ì¶”ê°€ (ì²´í¬í¬ì¸íŠ¸ìš©)
    train_config_dict = dict(train_config)
    train_config_dict["save_dir"] = str(save_dir)

    # Shuffled training ì‹¤í–‰
    losses = train_shuffled_documents(
        model=model,
        tokenized_docs=tokenized_docs,
        z_vectors=z_vectors,
        config=train_config_dict,
        scaler=scaler,
    )

    # ê²°ê³¼ ì €ì¥
    results = {
        "losses": losses,
        "num_docs": len(corpus),
        "config": OmegaConf.to_container(config),
    }

    # z_poolì— ì¶”ê°€
    for doc_id in tqdm(doc_ids_list, desc="Saving to z_pool"):
        z_pool_manager.add_z(doc_id, z_vectors[doc_id].detach())

    # ==========================================
    # 5. Final Save
    # ==========================================
    logger.info("\n[Step 5] Saving Results")

    # z_pool ì €ì¥ (Phase 3ì—ì„œ ë¡œë“œí•  ë©”ì¸ íŒŒì¼)
    z_pool_path = save_dir / "z_pool.pt"
    z_pool_manager.save(z_pool_path)

    # Projection layer ì €ì¥
    proj_path = save_dir / "projection.pt"
    model.save_projection(proj_path)

    # Results ì €ì¥
    results["avg_loss"] = sum(results["losses"].values()) / len(results["losses"])
    torch.save(results, save_dir / "results.pt")

    logger.info(f"\nFinal Average Loss: {results['avg_loss']:.4f}")
    logger.info(f"Saved z_pool to: {z_pool_path}")
    logger.info(f"Saved projection to: {proj_path}")

    # ==========================================
    # 5.1 Corpus Manifest ì €ì¥ (ë°ì´í„° ë™ì¼ì„± ê²€ì¦ìš©)
    # ==========================================
    import hashlib
    import json

    corpus_manifest = {
        "created_at": str(torch.cuda.current_device()) if torch.cuda.is_available() else "cpu",
        "num_docs": len(doc_ids_list),
        "documents": {}
    }

    for doc_id in tqdm(doc_ids_list, desc="Creating manifest"):
        text = corpus[doc_id]
        text_hash = hashlib.sha256(text.encode('utf-8')).hexdigest()
        first_16_tokens = tokenized_docs[doc_id]["input_ids"][0, :16].tolist()

        corpus_manifest["documents"][doc_id] = {
            "text_sha256": text_hash,
            "text_len_chars": len(text),
            "first_16_tokens": first_16_tokens,
        }

    manifest_path = save_dir / "corpus_manifest.json"
    with open(manifest_path, "w") as f:
        json.dump(corpus_manifest, f, indent=2)
    logger.info(f"Saved corpus manifest to: {manifest_path}")

    # ==========================================
    # 6. Validation: Generate from z + Keyword Check
    # ==========================================
    logger.info("\n[Step 6] Validation: Generate from learned z")

    # Projection ìƒíƒœ í™•ì¸
    proj_frozen = not any(p.requires_grad for p in model.z_to_embedding.parameters())
    logger.info(f"Projection layer: {'FROZEN' if proj_frozen else 'TRAINABLE'}")
    logger.info(f"Final alpha value: {model.alpha.item():.4f}")

    # z_pool í†µê³„
    z_pool_tensor = z_pool_manager.get_pool_tensor()
    logger.info(f"z_pool shape: {z_pool_tensor.shape}")
    logger.info(f"z_pool stats: mean={z_pool_tensor.mean():.4f}, std={z_pool_tensor.std():.4f}, "
               f"min={z_pool_tensor.min():.4f}, max={z_pool_tensor.max():.4f}")

    def extract_keywords(text: str, top_n: int = 10) -> set:
        """ë¬¸ì„œì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨ ë²„ì „: ê¸¸ì´ 4+ ë‹¨ì–´)"""
        import re
        words = re.findall(r'\b[a-zA-Z]{4,}\b', text.lower())
        # ë¹ˆë„ìˆœ ì •ë ¬
        from collections import Counter
        word_counts = Counter(words)
        # stopwords ì œì™¸
        stopwords = {'this', 'that', 'with', 'from', 'have', 'were', 'been', 'their', 'which', 'would', 'could', 'should', 'there', 'where', 'when', 'what', 'about', 'into', 'more', 'some', 'also', 'than', 'them', 'then', 'only', 'over', 'such', 'just', 'like', 'being', 'other', 'very', 'after', 'most', 'make', 'made', 'well', 'back', 'even', 'want', 'give', 'because', 'these', 'first', 'your', 'said'}
        filtered = [(w, c) for w, c in word_counts.most_common(top_n * 2) if w not in stopwords]
        return set(w for w, c in filtered[:top_n])

    def check_keyword_overlap(original: str, generated: str) -> tuple:
        """ì›ë¬¸ê³¼ ìƒì„±ë¬¸ì˜ í‚¤ì›Œë“œ ê²¹ì¹¨ í™•ì¸"""
        orig_kw = extract_keywords(original)
        gen_kw = extract_keywords(generated)
        if not orig_kw:
            return 0.0, set(), set()
        overlap = orig_kw & gen_kw
        ratio = len(overlap) / len(orig_kw)
        return ratio, overlap, orig_kw

    # ëª‡ ê°œ ìƒ˜í”Œ ìƒì„±
    num_samples = min(3, len(doc_ids_list))
    total_keyword_score = 0.0

    logger.info(f"Generating {num_samples} validation samples...")
    for i in tqdm(range(num_samples), desc="Validation samples"):
        doc_id = doc_ids_list[i]
        z_i = z_pool_manager.get_z(doc_id).to(model.device)

        # z_i í†µê³„
        logger.info(f"\n--- Sample {i+1}: {doc_id} ---")
        logger.info(f"z_i shape: {z_i.shape}, dtype: {z_i.dtype}")
        logger.info(f"z_i stats: mean={z_i.mean():.4f}, std={z_i.std():.4f}, norm={z_i.norm():.4f}")

        # ìƒ˜í”Œë§ê³¼ greedy ë‘˜ ë‹¤ í…ŒìŠ¤íŠ¸
        generated_sample = model.generate_from_z(z_i, max_new_tokens=128, do_sample=True)
        generated_greedy = model.generate_from_z(z_i, max_new_tokens=128, do_sample=False)
        original = corpus[doc_id]

        # z_embed í†µê³„
        stats = model.get_z_embed_stats(z_i)

        logger.info(f"z_embed stats: norm={stats['z_embed_norm']:.4f}, mean={stats['z_embed_mean']:.4f}, std={stats['z_embed_std']:.4f}")
        logger.info(f"Original (first 200 chars): {original[:200]}...")
        logger.info(f"Generated (sampling): {generated_sample[:200]}...")
        logger.info(f"Generated (greedy):   {generated_greedy[:200]}...")

        # Keyword overlap check (objective verification)
        kw_ratio, overlap, orig_kw = check_keyword_overlap(original, generated_sample)
        total_keyword_score += kw_ratio
        logger.info(f"Keyword check: {kw_ratio:.1%} overlap ({len(overlap)}/{len(orig_kw)})")
        logger.info(f"  Original keywords: {list(orig_kw)[:5]}...")
        logger.info(f"  Matched keywords:  {list(overlap)}")

    # ì „ì²´ keyword ì ìˆ˜
    avg_keyword_score = total_keyword_score / num_samples if num_samples > 0 else 0
    logger.info(f"\n[Objective] Avg keyword overlap: {avg_keyword_score:.1%}")
    results["avg_keyword_score"] = avg_keyword_score

    logger.info("\n" + "=" * 60)
    logger.info("Phase 1 Training Complete!")
    logger.info("=" * 60)

    return model, z_pool_manager, results


def main():
    """CLI entry point"""
    parser = argparse.ArgumentParser(description="Phase 1: Write Phase Training")
    parser.add_argument(
        "--config",
        type=str,
        default="configs/phase1_write.yaml",
        help="Path to config file",
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="Run in test mode (small scale)",
    )
    args = parser.parse_args()

    run_write_phase_training(config_path=args.config, test_mode=args.test)


if __name__ == "__main__":
    main()
